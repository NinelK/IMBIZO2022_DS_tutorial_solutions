{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NinelK/SA_DS_tutorial/blob/main/DS_Tutorial_Part_4_SOLUTIONS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYnZpaluodGM"
      },
      "source": [
        "## Setups"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQWtwVarOAha"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "import h5py\n",
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "from jax.experimental import optimizers\n",
        "from jax.config import config\n",
        "from jax.nn import relu\n",
        "from jax.random import poisson as jpoisson\n",
        "#config.update(\"jax_debug_nans\", True) # Useful for finding numerical errors\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np  # original CPU-backed NumPy\n",
        "import scipy.signal\n",
        "import scipy.stats\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "from importlib import reload\n",
        "from tqdm import tqdm\n",
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "start = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-YGOft2OzXh"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "! pip install \"dandi>=0.13.0\"\n",
        "! pip install git+https://github.com/neurallatents/nlb_tools.git\n",
        "! git clone -b SA_tutorial https://github.com/NinelK/jax-lfads.git\n",
        "# ! rm -r /content/jax-lfads/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6yb32yVUOY9i"
      },
      "outputs": [],
      "source": [
        "# You must change this to the location of the computation-thru-dynamics directory.\n",
        "HOME_DIR = '/content' \n",
        "\n",
        "sys.path.append(os.path.join(HOME_DIR,'jax-lfads'))\n",
        "import lfads_tutorial.lfads as lfads\n",
        "import lfads_tutorial.plotting as plotting\n",
        "import lfads_tutorial.utils as utils\n",
        "from lfads_tutorial.optimize import optimize_lfads, get_kl_warmup_fun\n",
        "from lfads_tutorial.lfads import sigmoid\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from nlb_tools.nwb_interface import NWBDataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJfOqmLlQaLO"
      },
      "outputs": [],
      "source": [
        "# Make directories\n",
        "lfads_dir = '/content/lfads/'       # where to save lfads data and parameters to\n",
        "output_dir = os.path.join(lfads_dir, 'output/')\n",
        "figure_dir = os.path.join(lfads_dir, os.path.join(output_dir, 'figures/'))\n",
        "if not os.path.exists(lfads_dir):\n",
        "    os.makedirs(output_dir)\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "if not os.path.exists(figure_dir):\n",
        "    os.makedirs(figure_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMrgtzJfmhSY"
      },
      "source": [
        "# Part 4: Neural population dynamics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsDEYHrjmi7j"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "In recent years, progress in recording technologies enabled recordings of 100-1000-10000 of neurons simultaneously.\n",
        "\n",
        "At the same time, it was shown that neural population activity often has a low-dimensional structure: a low number of latent dynamical factors can explain a large fraction of neural variability. This finding is called a 'manifold hypothesis', and was proposed in [Vyas et al. 2020](#references).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eiT5yEPP52a-"
      },
      "source": [
        "## Downloading MC_MAZE dataset from DANDI repository"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrFWwuqitfPQ"
      },
      "source": [
        "![MC_Maze](https://neurallatents.github.io/assets/maze_fig1.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjVesUYxuUgd"
      },
      "source": [
        "The maze task is a delayed center-out reach task with barriers, resulting in a variety of straight and curved trajectories. Neural activity was recorded from the dorsal premotor and primary motor cortices, and cursor, monkey gaze position, and monkey hand position and velocity are also provided.\n",
        "\n",
        "It has been found that M1/PMd activity during such planned, highly-stereotyped movements is predictable from neural population state at movement onset. Though this does not imply the neural system is itself autonomous, predictability makes the dataset useful for evaluating a methodâ€™s ability to model autonomous dynamics. In addition to a standard session, we provide 3 recording sessions (standard, large, medium, and small) with varying numbers of trials in order to test how modeling methods scale to limited data.\n",
        "\n",
        "[*Source: https://neurallatents.github.io/datasets.html*]\n",
        "\n",
        "The 4 recording sessions released on DANDI are provided by Matt Kaufman, Mark Churchland, and Krishna Shenoy at Stanford University."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nP0333gYvH4q"
      },
      "source": [
        "Downloading the dataset from DANDI takes just 1 line:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3LRF_pKvEs1"
      },
      "outputs": [],
      "source": [
        "# ! dandi download DANDI:000140/0.220113.0408 # Small\n",
        "# ! dandi download DANDI:000138/0.220113.0407 # Large\n",
        "# ! dandi download https://dandiarchive.org/dandiset/000129/draft\n",
        "# ! dandi download https://dandiarchive.org/dandiset/000070/draft\n",
        "! dandi download DANDI:000128/0.220113.0400 # Standard (should be called EXTRA-SUPER-LARGE-700Mb dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXjIFA2vvNRA"
      },
      "source": [
        "Because the data in DANDI repositories is standardised according to Neuroscience Without Borders (NWB) standards, we can use ready-made data loaders for this format:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "786mj14UkZng"
      },
      "outputs": [],
      "source": [
        "# we are using a dataloader from NLB tools \n",
        "dataset = NWBDataset(\"/content/000128/sub-Jenkins\", \"*train\", \n",
        "                     split_heldout=False)\n",
        "\n",
        "bin = 10 # [ms]\n",
        "dataset.resample(bin)\n",
        "\n",
        "# to view the dataset, uncomment the next line\n",
        "# dataset.data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuTHCY_mxKRG"
      },
      "source": [
        "Let us get familiar with the dataset. How many trials does it have?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s4XA3ZOWsmA7"
      },
      "outputs": [],
      "source": [
        "dataset.trial_info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paFQoGRTnrtD"
      },
      "source": [
        "## Preparing data for analysing trial-average responses"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exploring different experimental conditions"
      ],
      "metadata": {
        "id": "uLRFgNF9c6RB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The continuous data provided with the MC_Maze datasets includes:\n",
        "\n",
        "* `cursor_pos` - x and y position of the cursor controlled by the monkey\n",
        "* `eye_pos` - x and y position of the monkey's point of gaze on the screen, in mm\n",
        "* `hand_pos` - x and y position of the monkey's hand, in mm\n",
        "* `hand_vel` - x and y velocities of the monkey's hand, in mm/s, computed offline using np.gradient\n",
        "* `spikes` - spike times binned at 1 ms\n",
        "\n",
        "(You can see all these keys in `dataset.data` dataframe)\n",
        "\n",
        "Here we will only use a single aspect of behavior (e.g. `hand_pos`) for visualizing different trial conditions, while our analysis will focus on spike data `spikes`."
      ],
      "metadata": {
        "id": "ZhEKJJJ5y8cc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spikes_key = 'spikes'\n",
        "behavior_key = 'hand_pos'"
      ],
      "metadata": {
        "id": "VXOMjRZaXXn-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The full MC_Maze dataset has 108 (!) different reach conditions. Here, we'll plot the average trajectory per condition to see what typical reaches look like:\n",
        "\n"
      ],
      "metadata": {
        "id": "i8u-lw47W8sI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot trial-averaged reaches (code from: \n",
        "# https://github.com/neurallatents/neurallatents.github.io/blob/master/notebooks/mc_maze.ipynb)\n",
        "\n",
        "# Find unique conditions\n",
        "conds = dataset.trial_info.set_index(['trial_type', 'trial_version']).index.unique().tolist()\n",
        "\n",
        "print('Conditions #: ',len(conds))\n",
        "\n",
        "# Initialize plot\n",
        "fig = plt.figure(figsize=(6, 6))\n",
        "ax = fig.add_axes([0.1, 0.1, 0.8, 0.8])\n",
        "\n",
        "# Loop over conditions and compute average trajectory\n",
        "for cond in conds:\n",
        "    # Find trials in condition\n",
        "    mask = np.all(dataset.trial_info[['trial_type', 'trial_version']] == cond, axis=1)\n",
        "    # Extract trial data\n",
        "    trial_data = dataset.make_trial_data(align_field='move_onset_time', align_range=(-50, 450), ignored_trials=(~mask))\n",
        "    # Average hand position across trials\n",
        "    traj = trial_data.groupby('align_time')[[('hand_pos', 'x'), ('hand_pos', 'y')]].mean().to_numpy()\n",
        "    # Determine reach angle for color\n",
        "    active_target = dataset.trial_info[mask].target_pos.iloc[0][dataset.trial_info[mask].active_target.iloc[0]]\n",
        "    reach_angle = np.arctan2(*active_target[::-1])\n",
        "    # Plot reach\n",
        "    ax.plot(traj[:, 0], traj[:, 1], linewidth=0.7, color=plt.cm.hsv(reach_angle / (2*np.pi) + 0.5))\n",
        "\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nodVdBF5Xl8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trajectories here are colored according to the angle pointing to the reach target. Because of the maze barriers, reach trajectories are often curved."
      ],
      "metadata": {
        "id": "v-vkIoL2bpnc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Getting firing rates"
      ],
      "metadata": {
        "id": "dQScsyGfc1Zq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Smooth spikes with 50 ms std Gaussian\n",
        "dataset.smooth_spk(50, name='smth_50')"
      ],
      "metadata": {
        "id": "fIU1YNVrx3Yp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculating average firing rates for a subset of conditions"
      ],
      "metadata": {
        "id": "H_VmKUNEyKqC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Seed generator for consistent plots\n",
        "np.random.seed(42)\n",
        "n_conds = 30 # number of conditions to plot\n",
        "\n",
        "# Loop through conditions\n",
        "rates = []\n",
        "colors = []\n",
        "for i in np.random.choice(len(conds), n_conds):\n",
        "    cond = conds[i]\n",
        "    # Find trials in condition\n",
        "    mask = np.all(dataset.trial_info[['trial_type', 'trial_version']] == cond, axis=1)\n",
        "    # Extract trial data\n",
        "    trial_data = dataset.make_trial_data(align_field='move_onset_time', align_range=(-50, 450), ignored_trials=(~mask))\n",
        "    # Append averaged smoothed spikes for condition\n",
        "    rates.append(trial_data.groupby('align_time')[trial_data[['spikes_smth_50']].columns].mean().to_numpy())\n",
        "    # Append reach angle-based color for condition\n",
        "    active_target = dataset.trial_info[mask].target_pos.iloc[0][dataset.trial_info[mask].active_target.iloc[0]]\n",
        "    reach_angle = np.arctan2(*active_target[::-1])\n",
        "    colors.append(plt.cm.hsv(reach_angle / (2*np.pi) + 0.5))\n",
        "\n",
        "# Stack and scale data\n",
        "rate_stack = np.vstack(rates)\n",
        "rate_scaled = StandardScaler().fit_transform(rate_stack) # each neuron's firing rate ~ N(0,1)"
      ],
      "metadata": {
        "id": "bxMCdFX2ydKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qd1VL4awm9mm"
      },
      "source": [
        "### Principle component analysis (PCA)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neural population activity has a lot of dimensions: as many as there are neurons recorded!\n",
        "\n",
        "The usual first step for *explorative* analysis of neural data is reducing the dimensionality with a linear method: a Principle Component Analysis (PCA).\n",
        "\n",
        "PCA transforms the original trial-average data $D$, which is a *\\[neurons x time\\]* matrix into a smaller *\\[components x time\\]* matrix $A$:\n",
        "\n",
        "$$ D_{[N \\times T]} = S_{[N \\times M]} A_{[M \\times T]}$$\n",
        "\n",
        "Similarly to the way we aligned the "
      ],
      "metadata": {
        "id": "5fn0U2B2l6U6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "1S6wF9OvzpV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qTKkKWkKmgQE"
      },
      "outputs": [],
      "source": [
        "## Plot neural trajectories for subset of conditions\n",
        "\n",
        "pca = PCA(n_components=3)\n",
        "traj_stack = pca.fit_transform(rate_scaled)\n",
        "traj_arr = traj_stack.reshape((n_conds, len(rates[0]), -1))\n",
        "\n",
        "# Loop through trajectories and plot\n",
        "fig = plt.figure(figsize=(9, 6))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "for traj, col in zip(traj_arr, colors):\n",
        "    ax.plot(traj[:, 0], traj[:, 1], traj[:, 2], color=col)\n",
        "    ax.scatter(traj[0, 0], traj[0, 1], traj[0, 2], color=col) \n",
        "\n",
        "# Add labels\n",
        "ax.set_xlabel('PC1')\n",
        "ax.set_ylabel('PC2')\n",
        "ax.set_zlabel('PC3')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summary\n",
        "\n",
        "1. We have learned how to download and pre-process data in NWB format. \n",
        "2. We learned how dimensionality reduction summarizes what is going on in neural populations.\n",
        "3. We found rotation dynamics using trial average neural responses.\n",
        "\n",
        "But can we detect rotation dynamics from single-trial data directly, *without* knowing conditions?"
      ],
      "metadata": {
        "id": "7Xk-ORsIdOFI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1oUSJD_LtKm"
      },
      "source": [
        "## Single-trial analysis: inferring dynamics & initial conditions using LFADS\n",
        "\n",
        "Auto-encoding sequential neural data with RNN-based LFADS model "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Align and slice the data as [trials x time x neurons]"
      ],
      "metadata": {
        "id": "9EbAFJRHFLGI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aCF02Srbm4ne"
      },
      "outputs": [],
      "source": [
        "# Extract neural data and lagged hand velocity\n",
        "trial_length = 800//bin\n",
        "trial_data = dataset.make_trial_data(align_field='move_onset_time', \n",
        "                                     align_range=(-200, -200+bin*trial_length))\n",
        "\n",
        "print(trial_data['trial_id'].to_numpy().reshape((-1,trial_length)))\n",
        "spikes = trial_data[spikes_key].to_numpy() \n",
        "behavior = trial_data[behavior_key].to_numpy()\n",
        "\n",
        "spikes = spikes.reshape((-1,trial_length,spikes.shape[-1])) # [trials x time x inputs]\n",
        "behavior = behavior.reshape((-1,trial_length,behavior.shape[-1])) # [trials x time x outputs]\n",
        " \n",
        "data_bxtxn = spikes\n",
        "data_bxtxn.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lBEJMxBylGo5"
      },
      "outputs": [],
      "source": [
        "data_dt = 10.0        # define our dt in a physiological range [ms]\n",
        "\n",
        "train_fraction = 0.9      # Train with 90% of the synthetic data\n",
        "\n",
        "nexamples, ntimesteps, data_dim = data_bxtxn.shape\n",
        "\n",
        "train_data, eval_data = utils.split_data(data_bxtxn,\n",
        "                                         train_fraction=train_fraction)\n",
        "eval_data_offset = int(train_fraction * data_bxtxn.shape[0])\n",
        "\n",
        "# find reach directions for labeling\n",
        "conds = dataset.trial_info.set_index(['trial_type', 'trial_version']).index.unique().tolist()\n",
        "# Loop over conditions and compute average trajectory\n",
        "reach_angle = np.empty(len(dataset.trial_info)).astype('float32')\n",
        "for cond in conds:\n",
        "    # Find trials in condition\n",
        "    mask = np.all(dataset.trial_info[['trial_type', 'trial_version']] == cond, axis=1)\n",
        "    # Determine reach angle for color\n",
        "    active_target = dataset.trial_info[mask].target_pos.iloc[0][dataset.trial_info[mask].active_target.iloc[0]]\n",
        "    reach_angle[mask] = np.arctan2(*active_target[::-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Check basic stats"
      ],
      "metadata": {
        "id": "QRyRVTNZFXv0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yMapWLsOm6oB"
      },
      "outputs": [],
      "source": [
        "plt.plot(behavior.std(0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hk6vMIBnyVbW"
      },
      "outputs": [],
      "source": [
        "eval_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6yCeoZtnFti"
      },
      "outputs": [],
      "source": [
        "import sklearn\n",
        "ncomponents = data_dim\n",
        "full_pca = sklearn.decomposition.PCA(ncomponents)\n",
        "full_pca.fit(np.reshape(data_bxtxn, [-1, data_dim]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gtkl8AvvA7RP"
      },
      "outputs": [],
      "source": [
        "plt.scatter(*full_pca.components_[:2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zIokKxsNnzCZ"
      },
      "outputs": [],
      "source": [
        "plt.stem(full_pca.explained_variance_)\n",
        "plt.title('Classic exponential spectrum');"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set LFADS Parameters"
      ],
      "metadata": {
        "id": "PI-CTJ0cRWKs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPH3GH-Dnkmd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "outputId": "6b7a1a68-1855-4335-9426-a9de534f1458"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-7956d5f2c3b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# LFADS Hyper parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# input to lfads should have dimensions:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mntimesteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#   (batch_size x ntimesteps x data_dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m      \u001b[0;31m# batch size during optimization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_data' is not defined"
          ]
        }
      ],
      "source": [
        "# LFADS Hyper parameters\n",
        "data_dim = train_data.shape[2]  # input to lfads should have dimensions:\n",
        "ntimesteps = train_data.shape[1] #   (batch_size x ntimesteps x data_dim)\n",
        "batch_size = 200      # batch size during optimization\n",
        "\n",
        "# LFADS architecture - The size of the numbers is rather arbitrary, \n",
        "# but relatively small because we know the integrator RNN isn't too high \n",
        "# dimensional in its activity.\n",
        "enc_dim = 16         # encoder dim\n",
        "con_dim = 0          # controller dim\n",
        "ii_dim = 0           # inferred input dim\n",
        "gen_dim = 16         # generator dim, should be large enough\n",
        "factors_dim = 2      # factors dim, should be large enough to capture most variance of dynamics\n",
        "\n",
        "# Numerical stability\n",
        "var_min = 0.001 # Minimal variance any gaussian can become.\n",
        "\n",
        "# Optimization HPs that percolates into model\n",
        "l2reg = 0.00002\n",
        "\n",
        "# Initial state prior parameters\n",
        "# the mean is set to zero in the code\n",
        "ic_prior_var = 0.1 # this is $\\sigma^2_p$ in above paragraph\n",
        "\n",
        "# Inferred input autoregressive prior parameters\n",
        "ar_mean = 0.0                 # process mean\n",
        "ar_autocorrelation_tau = 1.0  # seconds, how correlated each time point is, related to $\\phi$ above.\n",
        "ar_noise_variance = 0.1       # noise variance\n",
        "\n",
        "lfads_hps = {'data_dim' : data_dim, 'ntimesteps' : ntimesteps,\n",
        "             'enc_dim' : enc_dim, 'con_dim' : con_dim, 'var_min' : var_min,\n",
        "             'ic_prior_var' : ic_prior_var, 'ar_mean' : ar_mean,\n",
        "             'ar_autocorrelation_tau' : ar_autocorrelation_tau,\n",
        "             'ar_noise_variance' : ar_noise_variance,\n",
        "             'ii_dim' : ii_dim, 'gen_dim' : gen_dim,\n",
        "             'factors_dim' : factors_dim,\n",
        "             'l2reg' : l2reg,\n",
        "             'batch_size' : batch_size}\n",
        "\n",
        "num_batches = 10000         # how many batches do we train\n",
        "print_every = 100            # give information every so often\n",
        "\n",
        "# Learning rate HPs\n",
        "step_size = 0.01            # initial learning rate\n",
        "decay_factor = 0.9998       # learning rate decay param\n",
        "decay_steps = 1             # learning rate decay param\n",
        "\n",
        "# Regularization HPs\n",
        "keep_rate = 0.95             # dropout keep rate during training\n",
        "\n",
        "# Numerical stability HPs\n",
        "max_grad_norm = 1.0        # gradient clipping above this value\n",
        "\n",
        "# The fact that the start and end values are required to be floats is something I need to fix.\n",
        "kl_warmup_start = 0.0 # batch number to start KL warm-up, explicitly float\n",
        "kl_warmup_end = 1000.0  # batch number to be finished with KL warm-up, explicitly float\n",
        "kl_min = 0.001 # The minimum KL value, non-zero to make sure KL doesn't grow crazy before kicking in.\n",
        "kl_max = 1.\n",
        "\n",
        "lfads_opt_hps = {'num_batches' : num_batches, 'step_size' : step_size,\n",
        "                 'decay_steps' : decay_steps, 'decay_factor' : decay_factor,\n",
        "                 'kl_min' : kl_min, 'kl_max' : kl_max, 'kl_warmup_start' : kl_warmup_start,\n",
        "                 'kl_warmup_end' : kl_warmup_end, 'keep_rate' : keep_rate,\n",
        "                 'max_grad_norm' : max_grad_norm, 'print_every' : print_every,\n",
        "                 'adam_b1' : 0.9, 'adam_b2' : 0.999, 'adam_eps' : 1e-4}\n",
        "\n",
        "class hashabledict(dict):\n",
        "    def __hash__(self):\n",
        "        return hash(tuple(sorted(self.items())))\n",
        "\n",
        "lfads_hps = hashabledict(lfads_hps)\n",
        "lfads_opt_hps = hashabledict(lfads_opt_hps)\n",
        "\n",
        "assert num_batches >= print_every and num_batches % print_every == 0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialize LFADS"
      ],
      "metadata": {
        "id": "SlfuSm79Ra1A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "hqWX6Kl4oGCq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "outputId": "393fd7d8-d29e-4410-ad56-1120400e3a3d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-e42ffc0f951c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32melif\u001b[0m \u001b[0mrnn_type\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'gru'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0;31m# to get the results like in Nature Methods\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m   \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlfads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgru\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m   \u001b[0mmodel_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlfads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgru_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'lfads' is not defined"
          ]
        }
      ],
      "source": [
        "# Initialize parameters for LFADS\n",
        "from functools import partial\n",
        "\n",
        "rnn_type = 'gru'\n",
        "if rnn_type=='vrnn':\n",
        "  nonlin = lambda x: jnp.tanh(x)\n",
        "  model = partial(vanilla_rnn, **{'nonlin': nonlin})\n",
        "  model_params = vanilla_rnn_params\n",
        "elif rnn_type=='vrnn_relu':\n",
        "  nonlin = lambda x: relu(x)\n",
        "  model = partial(vanilla_rnn, **{'nonlin': nonlin})\n",
        "  model_params = vanilla_rnn_params\n",
        "elif rnn_type=='gru':\n",
        "  # to get the results like in Nature Methods\n",
        "  model = lfads.gru\n",
        "  model_params = lfads.gru_params\n",
        "else:\n",
        "  raise ValueError('Unknown type')\n",
        "  \n",
        "key = random.PRNGKey(np.random.randint(0, utils.MAX_SEED_INT))\n",
        "init_params = lfads.lfads_params(key, lfads_hps, gen_rnn_params=model_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Option 1: Train LFADS"
      ],
      "metadata": {
        "id": "jSGpt0EbRdHE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ruJdYukooPT"
      },
      "outputs": [],
      "source": [
        "# Takes 12 minutes to train on a GPU; first step takes ~20 sec.\n",
        "key = random.PRNGKey(np.random.randint(0, utils.MAX_SEED_INT))\n",
        "trained_params, opt_details = \\\n",
        "    optimize_lfads(key, init_params, lfads_hps, lfads_opt_hps,\\\n",
        "                   train_data, eval_data, gen=model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhrKd0T_pFEC"
      },
      "outputs": [],
      "source": [
        "fname_uniquifier = datetime.datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
        "network_fname = (f'trained_params_lfads_pend_{rnn_type}' + fname_uniquifier + '.npz')\n",
        "network_path = os.path.join(output_dir, network_fname)\n",
        "\n",
        "print(\"Saving parameters: \", network_path)\n",
        "np.savez(network_path, trained_params)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JLo5QVPM76Eh"
      },
      "outputs": [],
      "source": [
        "# Plot the training details\n",
        "x = np.arange(0, num_batches, print_every)\n",
        "plt.figure(figsize=(20,6))\n",
        "plt.subplot(251)\n",
        "plt.plot(x, opt_details['tlosses']['total'], 'k')\n",
        "plt.ylabel('Training')\n",
        "plt.title('Total loss')\n",
        "plt.subplot(252)\n",
        "plt.plot(x, opt_details['tlosses']['nlog_p_xgz'], 'b')\n",
        "plt.title('Negative log p(z|x)')\n",
        "plt.subplot(253)\n",
        "plt.plot(x, opt_details['tlosses']['kl_ii'], 'r')\n",
        "plt.title('KL inferred inputs')\n",
        "plt.subplot(254)\n",
        "plt.plot(x, opt_details['tlosses']['kl_g0'], 'g')\n",
        "plt.title('KL initial state')\n",
        "plt.subplot(255)\n",
        "plt.plot(x, opt_details['tlosses']['l2'], 'c')\n",
        "plt.xlabel('Training batch')\n",
        "plt.title('L2 loss')\n",
        "plt.subplot(256)\n",
        "plt.plot(x, opt_details['elosses']['total'], 'k')\n",
        "plt.xlabel('Training batch')\n",
        "plt.ylabel('Evaluation')\n",
        "plt.subplot(257)\n",
        "plt.plot(x, opt_details['tlosses']['nlog_p_xgz'], 'b')\n",
        "plt.xlabel('Training batch')\n",
        "plt.subplot(258)\n",
        "plt.plot(x, opt_details['elosses']['kl_ii'], 'r')\n",
        "plt.xlabel('Training batch')\n",
        "plt.subplot(259)\n",
        "plt.plot(x, opt_details['elosses']['kl_g0'], 'g')\n",
        "plt.xlabel('Training batch');"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Option 2: Load pre-trained model from file"
      ],
      "metadata": {
        "id": "f2Flvb6ujq98"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # After training, you can load the weights up, after locating the save file.\n",
        "# # network_fname = 'trained_params_lfads_128_vrnn2022-08-05_14:02:03.npz'\n",
        "# network_fname = 'trained_params_lfads_128_gru2022-08-05_12:54:43.npz'\n",
        "# network_path = os.path.join(output_dir, network_fname)\n",
        "\n",
        "# loaded_params = np.load(network_path, allow_pickle=True)\n",
        "# trained_params = loaded_params['arr_0'].item()\n"
      ],
      "metadata": {
        "id": "Df9qrzp0jp-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiEJTNiKQUOf"
      },
      "source": [
        "## Visualize latent space trajectories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lX7eD0ozQLmp"
      },
      "outputs": [],
      "source": [
        "# Plot a bunch of examples of eval trials run through LFADS.\n",
        "reload(plotting)\n",
        "#reload(lfads)\n",
        "\n",
        "def plot_rescale_fun(a): \n",
        "    fac = max_firing_rate * data_dt\n",
        "    return renormed_fun(a) * fac\n",
        "\n",
        "\n",
        "bidx = 0\n",
        "\n",
        "nexamples_to_save = 1\n",
        "for eidx in range(nexamples_to_save):\n",
        "    fkey = random.fold_in(key, eidx)\n",
        "    psa_example = eval_data[bidx,:,:].astype(jnp.float32)\n",
        "    psa_dict = lfads.posterior_sample_and_average_jit(trained_params, lfads_hps, \n",
        "                                                      fkey, psa_example, gen=model)\n",
        "\n",
        "    # The inferred input and true input are rescaled and shifted via \n",
        "    # linear regression to match, as there is an identifiability issue. there.\n",
        "    # plotting.plot_lfads(psa_example, psa_dict,\n",
        "    #                     data_dict, eval_data_offset+bidx, plot_rescale_fun)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JXh3d4vBQWBC"
      },
      "outputs": [],
      "source": [
        "# plt.plot(psa_dict['factor_t'])\n",
        "plt.plot(psa_dict['factor_t'])\n",
        "psa_dict.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tPgWndF3Qd7N"
      },
      "outputs": [],
      "source": [
        "# sample trajectories\n",
        "ics = np.empty((data_bxtxn.shape[0],gen_dim))\n",
        "gen_traj = np.empty((data_bxtxn.shape[0],ntimesteps,gen_dim))\n",
        "for i, psa_example in enumerate(data_bxtxn.astype(jnp.float32)):\n",
        "    psa_dict = lfads.posterior_sample_and_average_jit(trained_params, lfads_hps, \n",
        "                                                      fkey, psa_example, gen=model)\n",
        "    ics[i] = psa_dict['ic_mean']\n",
        "    gen_traj[i] = psa_dict['gen_t']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ics.shape"
      ],
      "metadata": {
        "id": "ydVNgikbKUIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QEK3_yJf3LKw"
      },
      "outputs": [],
      "source": [
        "ncomponents = 3\n",
        "pca = PCA(ncomponents)\n",
        "pca.fit(np.reshape(gen_traj, [-1, gen_dim]))\n",
        "\n",
        "S = pca.components_.T\n",
        "\n",
        "for a,t in zip(reach_angle,gen_traj):\n",
        "  plt.plot(*(t@S)[:,[1,2]].T, c=plt.cm.rainbow(a / (2*np.pi) + 0.5),alpha=0.5)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VVYuSVN95Fmx"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# print((gen_traj@S)[:,0])\n",
        "\n",
        "df = pd.DataFrame.from_dict({'PC0': (gen_traj@S)[::10,:,0].flatten(),\n",
        "                             'PC1': (gen_traj@S)[::10,:,1].flatten(),\n",
        "                             'PC2': (gen_traj@S)[::10,:,2].flatten(),\n",
        "                             'color': reach_angle[::10].repeat(ntimesteps).T})\n",
        "\n",
        "fig = px.scatter_3d(df, x=\"PC0\", y=\"PC1\", z=\"PC2\",color='color')\n",
        "fig.update_traces(marker=dict(size=3,\n",
        "                              line=dict(width=.01,\n",
        "                                        color='DarkSlateGrey')),\n",
        "                  selector=dict(mode='markers'))\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7BMhOvteRINg"
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "ncomponents = 3\n",
        "tsne = TSNE(ncomponents)#,perplexity=20)\n",
        "embedded_ics = tsne.fit_transform(ics)\n",
        "\n",
        "plt.scatter(*embedded_ics[:,:2].T, c=plt.cm.rainbow(reach_angle[:] / (2*np.pi) + 0.5),alpha=0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fixed point analysis"
      ],
      "metadata": {
        "id": "QoCQp4pRWTrl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "import fixed_point_finder.fixed_points as fp_optimize\n",
        "from jax import vmap\n",
        "import gc\n",
        "\n",
        "gc.collect() # free up some RAM"
      ],
      "metadata": {
        "id": "jZXOEIemX0dc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "UggIFlhsfIMx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# These are some preliminaries. \n",
        "x_star = np.zeros(ii_dim)  # We always linearize the input around zero in this example.\n",
        "\n",
        "# Make a one parameter function of thie hidden state, useful for jacobians.\n",
        "key = random.PRNGKey(np.random.randint(0, utils.MAX_SEED_INT))\n",
        "params = hashabledict(model_params(key, gen_dim, ii_dim))\n",
        "rnn_fun = lambda h : model(params, h, x_star)\n",
        "batch_rnn_fun = vmap(rnn_fun, in_axes=(0,))\n"
      ],
      "metadata": {
        "id": "G2wTGcuoW57E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create some functions that define the fixed point loss\n",
        "which is just the squared error of a point  for a discrete time system such as a VRNN or GRU."
      ],
      "metadata": {
        "id": "86n9OXzCXuSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fp_loss_fun = fp_optimize.get_fp_loss_fun(rnn_fun)\n",
        "total_fp_loss_fun = fp_optimize.get_total_fp_loss_fun(rnn_fun)"
      ],
      "metadata": {
        "id": "P-soJ5-MXsld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have to start the fixed point finder with some points, and it's always best to start with examples of where the state normally operates.\n"
      ],
      "metadata": {
        "id": "pY9w2VzWYRGN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fp_candidates = jnp.array(gen_traj[::50,])                        # was batch x time x dim\n",
        "fp_candidates = jnp.reshape(fp_candidates, (-1, gen_dim))  # now (batch * time) x dim"
      ],
      "metadata": {
        "id": "QyuLmgv5YPlP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fixed point optimization hyperparameters\n",
        "fp_num_batches = 1000         # Total number of batches to train on.\n",
        "fp_batch_size = 100          # How many examples in each batch\n",
        "fp_step_size = 0.2          # initial learning rate\n",
        "fp_decay_factor = 0.9999     # decay the learning rate this much\n",
        "fp_decay_steps = 1           #\n",
        "fp_adam_b1 = 0.9             # Adam parameters\n",
        "fp_adam_b2 = 0.999\n",
        "fp_adam_eps = 1e-5\n",
        "fp_opt_print_every = 200   # Print training information during optimziation every so often\n",
        "\n",
        "# Fixed point finding thresholds and other HPs\n",
        "fp_noise_var = 0.0      # Gaussian noise added to fixed point candidates before optimization.\n",
        "fp_opt_stop_tol = 0.00001  # Stop optimizing when the average value of the batch is below this value.\n",
        "fp_tol = 0.00001        # Discard fps with squared speed larger than this value.\n",
        "fp_unique_tol = 0.025      # tolerance for determination of identical fixed points\n",
        "fp_outlier_tol = 1.0    # Anypoint whos closest fixed point is greater than tol is an outlier.\n"
      ],
      "metadata": {
        "id": "10Q2MotSZFTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fp_candidates.shape"
      ],
      "metadata": {
        "id": "2L5z76zaQp5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reload(fp_optimize)\n",
        "\n",
        "fp_tols = [0.0001, 0.00001, 0.000001] # Used for both fp_tol and opt_stop_tol\n",
        "\n",
        "all_fps = {}\n",
        "for tol in fp_tols:\n",
        "  fp_hps = {'num_batches' : fp_num_batches, 'step_size' : fp_step_size, \n",
        "            'decay_factor' : fp_decay_factor, 'decay_steps' : fp_decay_steps, \n",
        "            'adam_b1' : fp_adam_b1, 'adam_b2' : fp_adam_b2,\n",
        "            'adam_eps' : fp_adam_eps, 'noise_var' : fp_noise_var, \n",
        "            'fp_opt_stop_tol' : tol, 'fp_tol' : tol, \n",
        "            'unique_tol' : fp_unique_tol, 'outlier_tol' : fp_outlier_tol, \n",
        "            'opt_print_every' : fp_opt_print_every}\n",
        "\n",
        "  fp_hps = hashabledict(fp_hps)\n",
        "\n",
        "  fps, fp_losses, fp_idxs, fp_opt_details = \\\n",
        "    fp_optimize.find_fixed_points(rnn_fun, fp_candidates, fp_hps, do_print=True)\n",
        "  if len(fp_idxs) > 0:\n",
        "      F_of_fps = batch_rnn_fun(fps)\n",
        "  else:\n",
        "      F_of_fps = np.zeros([0,gen_dim])\n",
        "      \n",
        "  all_fps[tol] = {'fps' : fps, 'candidates' : fp_candidates[fp_idxs],\n",
        "                  'losses' : fp_losses, 'F_of_fps' : F_of_fps, \n",
        "                  'opt_details' : fp_opt_details, 'hps' : fp_hps}\n",
        "\n",
        "  all_fps[tol]\n"
      ],
      "metadata": {
        "id": "JdmPOAmTZUpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "import pandas as pd\n",
        "\n",
        "# print((gen_traj@S)[:,0])\n",
        "\n",
        "df = pd.DataFrame.from_dict({'PC0': (gen_traj@S)[::10,:,0].flatten(),\n",
        "                             'PC1': (gen_traj@S)[::10,:,1].flatten(),\n",
        "                             'PC2': (gen_traj@S)[::10,:,2].flatten(),\n",
        "                             'color': reach_angle[::10].repeat(ntimesteps).T})\n",
        "\n",
        "fig = px.scatter_3d(df, x=\"PC0\", y=\"PC1\", z=\"PC2\",color='color')\n",
        "fig.update_traces(marker=dict(size=3,\n",
        "                              line=dict(width=.01,\n",
        "                                        color='DarkSlateGrey')),\n",
        "                  selector=dict(mode='markers'))\n",
        "\n",
        "fixed_points = all_fps[0.000001]['fps'] @ S\n",
        "print(fixed_points.shape)\n",
        "\n",
        "fig.add_trace(\n",
        "    go.Scatter3d(x=fixed_points[:,0],\n",
        "                 y=fixed_points[:,1],\n",
        "                 z=fixed_points[:,2],\n",
        "                 mode='markers')\n",
        ")\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "VOqqJJ-cIiSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fixed_points"
      ],
      "metadata": {
        "id": "Z5OH3d3yRnCm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 4: Linearizing around the fixed point: calculating the Jacobian"
      ],
      "metadata": {
        "id": "BtiOBMWALQOo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "jacs = fp_optimize.compute_jacobians(rnn_fun, all_fps[0.000001]['fps'])\n",
        "# eig_decomps = fp_optimize.compute_eigenvalue_decomposition(jacs, sort_by='real', do_compute_lefts=False)\n",
        "A_inferred = np.array(jacs[0])\n",
        "eig_decomps = np.linalg.eig(A_inferred)"
      ],
      "metadata": {
        "id": "EhXoQU4rNKBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(4,4))\n",
        "eigvals = eig_decomps[0]#['evals']\n",
        "plt.scatter(eigvals.real, eigvals.imag)\n",
        "x = jnp.linspace(-jnp.pi,jnp.pi,100)\n",
        "plt.plot(jnp.sin(x),jnp.cos(x),c='k')\n",
        "# plt.plot(x,jnp.cos(x))\n",
        "# scale = np.sqrt(eigvals.real**2 + eigvals.imag**2).max()*1.05\n",
        "scale=1\n",
        "plt.xlim([-scale,+scale])\n",
        "plt.ylim([-scale,+scale])"
      ],
      "metadata": {
        "id": "d15HIUaFMa_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "end = time.time()\n",
        "\n",
        "(end-start)/60"
      ],
      "metadata": {
        "id": "7Wi4XMRzZ_4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFRVmFtnnm36"
      },
      "source": [
        "## References\n",
        "<a name=\"references\"></a>\n",
        "0. [Computation through dynamics tutorial](https://github.com/google-research/computation-thru-dynamics) from David Sussillo, which contains an implementation of LFADS that was adapted to this tutorial + extra material (in depth LFADS tutorial, inferring unobserved inputs, FORCE training)\n",
        "\n",
        "1. *Manifold hypothesis:* Gallego, Juan A., et al. \"Neural manifolds for the control of movement.\" Neuron 94.5 (2017): 978-984.\n",
        "\n",
        "2. *A review of a population dynamics models:* Vyas, Saurabh, et al. \"Computation through neural population dynamics.\" Annual Review of Neuroscience 43 (2020): 249. [Not paywalled pdf.](https://web.stanford.edu/~mgolub/publications/2020-Vyas-ARN.pdf)\n",
        "\n",
        "3. *Our review on fully observed vs latent dynamical models*: Hurwitz, Cole, et al. [Building population models for large-scale neural recordings: Opportunities and pitfalls.](https://arxiv.org/pdf/2102.01807.pdf) Current Opinion in Neurobiology 70 (2021): 64-73.\n",
        "\n",
        "4. *For better understanding of PCA:* [Dimensionality reduction tutorial](https://compneuro.neuromatch.io/tutorials/W1D4_DimensionalityReduction/chapter_title.html) by Alex Cayco Gajic at Neuromatch Academy\n",
        "\n",
        "5. *The LFADS paper:* Pandarinath, Chethan, et al. [Inferring single-trial neural population dynamics using sequential auto-encoders.](https://www.nature.com/articles/s41592-018-0109-9) Nature methods 15.10 (2018): 805-815.\n",
        "\n",
        "6. *Critical view on rotational dynamics* Lebedev, Mikhail A., et al. \"Analysis of neuronal ensemble activity reveals the pitfalls and shortcomings of rotation dynamics.\" Scientific Reports 9.1 (2019): 1-14.\n",
        "  \n",
        "## Interesting recent papers for further reading\n",
        "\n",
        "If you want to know what's going on in the field right now, you can check the following papers (a very biased selection from Nina ;) ):\n",
        "\n",
        "1. Smith, Jimmy, Scott Linderman, and David Sussillo. [Reverse engineering recurrent neural networks with jacobian switching linear dynamical systems.](https://arxiv.org/pdf/2111.01256.pdf) Advances in Neural Information Processing Systems 34 (2021): 16700-16713.\n",
        "2. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project\n",
        "\n",
        "Pick another dataset (e.g. MC_RTT) from a [Neural Latents Benchmark](https://neurallatents.github.io/datasets.html) project. Follow the pre-processing notebook (available of NLB website) in order to align the data. Following the methods you learned today, analyse trial-average responses with dimensionality reduction techniques. Train a single-trial dynamical model to the data and analyse the latent dynamics (you can use the code from this notebook). \n",
        "\n",
        "Do your results agree with the type of computation that the neurons were supposed to perform?"
      ],
      "metadata": {
        "id": "syM_B3roOktI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "bctlWhTda_6O"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "DS Tutorial Part 4 SOLUTIONS",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP1K+oH7pbMWUji23GIbY5E",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}