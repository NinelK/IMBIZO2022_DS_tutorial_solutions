{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DS Tutorial Part 4 SOLUTIONS",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN+AhxYY9mMphOW7exFDlIL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NinelK/SA_DS_tutorial/blob/main/DS_Tutorial_Part_4_SOLUTIONS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 4: Recurrent neural networks (RNNs) in neuroscience"
      ],
      "metadata": {
        "id": "pN8Y1W5tnsk7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setups"
      ],
      "metadata": {
        "id": "PYnZpaluodGM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQWtwVarOAha",
        "outputId": "ed11f6e0-eacc-42ef-b0ca-8567623044a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/jax/experimental/optimizers.py:30: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead\n",
            "  FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "import datetime\n",
        "import h5py\n",
        "import jax.numpy as np\n",
        "from jax import random\n",
        "from jax.experimental import optimizers\n",
        "from jax.config import config\n",
        "#config.update(\"jax_debug_nans\", True) # Useful for finding numerical errors\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as onp  # original CPU-backed NumPy\n",
        "import scipy.signal\n",
        "import scipy.stats\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "from importlib import reload"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# ! pip install \"dandi>=0.13.0\"\n",
        "# ! dandi download DANDI:000140/0.220113.0408 # Small\n",
        "# ! dandi download DANDI:000138/0.220113.0407 # Large\n",
        "# ! dandi download https://dandiarchive.org/dandiset/000129/draft\n",
        "# ! dandi download https://dandiarchive.org/dandiset/000070/draft\n",
        "! pip install git+https://github.com/neurallatents/nlb_tools.git\n",
        "! git clone -b SA_tutorial https://github.com/NinelK/jax-lfads.git\n",
        "# ! git clone https://github.com/google-research/computation-thru-dynamics.git # needs Runtime restart?"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "x-YGOft2OzXh",
        "outputId": "1093ca27-9dc0-4896-f25a-2a3f56d3ac12"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: dandi>=0.13.0 in /usr/local/lib/python3.7/dist-packages (0.45.1)\n",
            "Requirement already satisfied: ruamel.yaml<1,>=0.15 in /usr/local/lib/python3.7/dist-packages (from dandi>=0.13.0) (0.17.21)\n",
            "Requirement already satisfied: fasteners in /usr/local/lib/python3.7/dist-packages (from dandi>=0.13.0) (0.17.3)\n",
            "Requirement already satisfied: nwbinspector>=0.3.8 in /usr/local/lib/python3.7/dist-packages (from dandi>=0.13.0) (0.4.12)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from dandi>=0.13.0) (7.1.2)\n",
            "Requirement already satisfied: pyout!=0.6.0,>=0.5 in /usr/local/lib/python3.7/dist-packages (from dandi>=0.13.0) (0.7.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from dandi>=0.13.0) (21.3)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from dandi>=0.13.0) (2.8.2)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.7/dist-packages (from dandi>=0.13.0) (1.4.4)\n",
            "Requirement already satisfied: click-didyoumean in /usr/local/lib/python3.7/dist-packages (from dandi>=0.13.0) (0.3.0)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.7/dist-packages (from dandi>=0.13.0) (0.5.1)\n",
            "Requirement already satisfied: etelemetry>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from dandi>=0.13.0) (0.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from dandi>=0.13.0) (4.12.0)\n",
            "Requirement already satisfied: dandischema~=0.7.0 in /usr/local/lib/python3.7/dist-packages (from dandi>=0.13.0) (0.7.1)\n",
            "Requirement already satisfied: keyring in /usr/local/lib/python3.7/dist-packages (from dandi>=0.13.0) (23.7.0)\n",
            "Requirement already satisfied: pynwb!=1.1.0,>=1.0.3 in /usr/local/lib/python3.7/dist-packages (from dandi>=0.13.0) (2.1.0)\n",
            "Requirement already satisfied: tenacity in /usr/local/lib/python3.7/dist-packages (from dandi>=0.13.0) (8.0.1)\n",
            "Requirement already satisfied: pydantic>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from dandi>=0.13.0) (1.9.1)\n",
            "Requirement already satisfied: pycryptodomex in /usr/local/lib/python3.7/dist-packages (from dandi>=0.13.0) (3.15.0)\n",
            "Requirement already satisfied: interleave~=0.1 in /usr/local/lib/python3.7/dist-packages (from dandi>=0.13.0) (0.2.1)\n",
            "Requirement already satisfied: requests~=2.20 in /usr/local/lib/python3.7/dist-packages (from dandi>=0.13.0) (2.23.0)\n",
            "Requirement already satisfied: zarr~=2.10 in /usr/local/lib/python3.7/dist-packages (from dandi>=0.13.0) (2.12.0)\n",
            "Requirement already satisfied: keyrings.alt in /usr/local/lib/python3.7/dist-packages (from dandi>=0.13.0) (4.1.1)\n",
            "Requirement already satisfied: semantic-version in /usr/local/lib/python3.7/dist-packages (from dandi>=0.13.0) (2.10.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from dandi>=0.13.0) (1.1.0)\n",
            "Requirement already satisfied: fscacher in /usr/local/lib/python3.7/dist-packages (from dandi>=0.13.0) (0.2.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from dandischema~=0.7.0->dandi>=0.13.0) (4.1.1)\n",
            "Requirement already satisfied: jsonschema[format] in /usr/local/lib/python3.7/dist-packages (from dandischema~=0.7.0->dandi>=0.13.0) (4.3.3)\n",
            "Requirement already satisfied: ci-info>=0.2 in /usr/local/lib/python3.7/dist-packages (from etelemetry>=0.2.2->dandi>=0.13.0) (0.3.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nwbinspector>=0.3.8->dandi>=0.13.0) (4.64.0)\n",
            "Requirement already satisfied: natsort in /usr/local/lib/python3.7/dist-packages (from nwbinspector>=0.3.8->dandi>=0.13.0) (5.5.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from nwbinspector>=0.3.8->dandi>=0.13.0) (3.13)\n",
            "Requirement already satisfied: email-validator>=1.0.3 in /usr/local/lib/python3.7/dist-packages (from pydantic>=1.9.0->dandi>=0.13.0) (1.2.1)\n",
            "Requirement already satisfied: dnspython>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from email-validator>=1.0.3->pydantic>=1.9.0->dandi>=0.13.0) (2.2.1)\n",
            "Requirement already satisfied: idna>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from email-validator>=1.0.3->pydantic>=1.9.0->dandi>=0.13.0) (2.10)\n",
            "Requirement already satisfied: h5py<4,>=2.10 in /usr/local/lib/python3.7/dist-packages (from pynwb!=1.1.0,>=1.0.3->dandi>=0.13.0) (3.1.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pynwb!=1.1.0,>=1.0.3->dandi>=0.13.0) (57.4.0)\n",
            "Requirement already satisfied: pandas<2,>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from pynwb!=1.1.0,>=1.0.3->dandi>=0.13.0) (1.3.5)\n",
            "Requirement already satisfied: hdmf<4,>=3.3.2 in /usr/local/lib/python3.7/dist-packages (from pynwb!=1.1.0,>=1.0.3->dandi>=0.13.0) (3.3.2)\n",
            "Requirement already satisfied: numpy<1.23,>=1.16 in /usr/local/lib/python3.7/dist-packages (from pynwb!=1.1.0,>=1.0.3->dandi>=0.13.0) (1.21.6)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py<4,>=2.10->pynwb!=1.1.0,>=1.0.3->dandi>=0.13.0) (1.5.2)\n",
            "Requirement already satisfied: scipy<2,>=1.1 in /usr/local/lib/python3.7/dist-packages (from hdmf<4,>=3.3.2->pynwb!=1.1.0,>=1.0.3->dandi>=0.13.0) (1.7.3)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema[format]->dandischema~=0.7.0->dandi>=0.13.0) (5.9.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema[format]->dandischema~=0.7.0->dandi>=0.13.0) (0.18.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema[format]->dandischema~=0.7.0->dandi>=0.13.0) (22.1.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema[format]->dandischema~=0.7.0->dandi>=0.13.0) (3.8.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas<2,>=1.0.5->pynwb!=1.1.0,>=1.0.3->dandi>=0.13.0) (2022.1)\n",
            "Requirement already satisfied: blessings in /usr/local/lib/python3.7/dist-packages (from pyout!=0.6.0,>=0.5->dandi>=0.13.0) (1.7)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil->dandi>=0.13.0) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests~=2.20->dandi>=0.13.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests~=2.20->dandi>=0.13.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests~=2.20->dandi>=0.13.0) (2022.6.15)\n",
            "Requirement already satisfied: ruamel.yaml.clib>=0.2.6 in /usr/local/lib/python3.7/dist-packages (from ruamel.yaml<1,>=0.15->dandi>=0.13.0) (0.2.6)\n",
            "Requirement already satisfied: asciitree in /usr/local/lib/python3.7/dist-packages (from zarr~=2.10->dandi>=0.13.0) (0.3.3)\n",
            "Requirement already satisfied: numcodecs>=0.6.4 in /usr/local/lib/python3.7/dist-packages (from zarr~=2.10->dandi>=0.13.0) (0.10.2)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from numcodecs>=0.6.4->zarr~=2.10->dandi>=0.13.0) (0.4)\n",
            "Requirement already satisfied: jsonpointer>1.13 in /usr/local/lib/python3.7/dist-packages (from jsonschema[format]->dandischema~=0.7.0->dandi>=0.13.0) (2.3)\n",
            "Requirement already satisfied: fqdn in /usr/local/lib/python3.7/dist-packages (from jsonschema[format]->dandischema~=0.7.0->dandi>=0.13.0) (1.5.1)\n",
            "Requirement already satisfied: isoduration in /usr/local/lib/python3.7/dist-packages (from jsonschema[format]->dandischema~=0.7.0->dandi>=0.13.0) (20.11.0)\n",
            "Requirement already satisfied: rfc3987 in /usr/local/lib/python3.7/dist-packages (from jsonschema[format]->dandischema~=0.7.0->dandi>=0.13.0) (1.3.8)\n",
            "Requirement already satisfied: rfc3339-validator in /usr/local/lib/python3.7/dist-packages (from jsonschema[format]->dandischema~=0.7.0->dandi>=0.13.0) (0.1.4)\n",
            "Requirement already satisfied: webcolors>=1.11 in /usr/local/lib/python3.7/dist-packages (from jsonschema[format]->dandischema~=0.7.0->dandi>=0.13.0) (1.12)\n",
            "Requirement already satisfied: uri-template in /usr/local/lib/python3.7/dist-packages (from jsonschema[format]->dandischema~=0.7.0->dandi>=0.13.0) (1.2.0)\n",
            "Requirement already satisfied: arrow>=0.15.0 in /usr/local/lib/python3.7/dist-packages (from isoduration->jsonschema[format]->dandischema~=0.7.0->dandi>=0.13.0) (1.2.2)\n",
            "Requirement already satisfied: SecretStorage>=3.2 in /usr/local/lib/python3.7/dist-packages (from keyring->dandi>=0.13.0) (3.3.2)\n",
            "Requirement already satisfied: jeepney>=0.4.2 in /usr/local/lib/python3.7/dist-packages (from keyring->dandi>=0.13.0) (0.8.0)\n",
            "Requirement already satisfied: cryptography>=2.0 in /usr/local/lib/python3.7/dist-packages (from SecretStorage>=3.2->keyring->dandi>=0.13.0) (37.0.4)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=2.0->SecretStorage>=3.2->keyring->dandi>=0.13.0) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=2.0->SecretStorage>=3.2->keyring->dandi>=0.13.0) (2.21)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->dandi>=0.13.0) (3.0.9)\n",
            "2022-08-02 13:11:22,811 [    INFO] NumExpr defaulting to 2 threads.\n",
            "PATH                                                              SIZE      DONE    DONE% CHECKSUM STATUS    MESSAGE    \n",
            "dandiset.yaml                                                                                      skipped   no change  \n",
            "sub-Jenkins/sub-Jenkins_ses-large_desc-test_ecephys.nwb                                                                 \n",
            "sub-Jenkins/sub-Jenkins_ses-large_desc-train_behavior+ecephys.nwb                                                       \n",
            "Summary:                                                          0 Bytes   0 Bytes                1 skipped 1 no change\n",
            "                                                                  +149.4 MB 0.00%                                       \n",
            "\n",
            "\n",
            "ERROR: 2 asynchronous workers failed\n",
            "\n",
            "Producing value for row ('sub-Jenkins/sub-Jenkins_ses-large_desc-test_ecephys.nwb',) failed:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyout/interface.py\", line 279, in _print_async_exceptions\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.7/concurrent/futures/_base.py\", line 428, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.7/concurrent/futures/_base.py\", line 384, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/lib/python3.7/concurrent/futures/thread.py\", line 57, in run\n",
            "    result = self.fn(*self.args, **self.kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyout/interface.py\", line 482, in async_fn\n",
            "    for i in gen:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/dandi/download.py\", line 521, in _download_file\n",
            "    raise FileExistsError(f\"File {path!r} already exists\")\n",
            "FileExistsError: File './000138/sub-Jenkins/sub-Jenkins_ses-large_desc-test_ecephys.nwb' already exists\n",
            "\n",
            "Producing value for row ('sub-Jenkins/sub-Jenkins_ses-large_desc-train_behavior+ecephys.nwb',) failed:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyout/interface.py\", line 279, in _print_async_exceptions\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.7/concurrent/futures/_base.py\", line 428, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.7/concurrent/futures/_base.py\", line 384, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/lib/python3.7/concurrent/futures/thread.py\", line 57, in run\n",
            "    result = self.fn(*self.args, **self.kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyout/interface.py\", line 482, in async_fn\n",
            "    for i in gen:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/dandi/download.py\", line 521, in _download_file\n",
            "    raise FileExistsError(f\"File {path!r} already exists\")\n",
            "FileExistsError: File './000138/sub-Jenkins/sub-Jenkins_ses-large_desc-train_behavior+ecephys.nwb' already exists\n",
            "\n",
            "2022-08-02 13:11:25,615 [    INFO] Logs saved in /root/.cache/dandi-cli/log/20220802131122Z-505.log\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/neurallatents/nlb_tools.git\n",
            "  Cloning https://github.com/neurallatents/nlb_tools.git to /tmp/pip-req-build-zm3b9w3a\n",
            "  Running command git clone -q https://github.com/neurallatents/nlb_tools.git /tmp/pip-req-build-zm3b9w3a\n",
            "Collecting pandas<=1.3.4,>=1.0.0\n",
            "  Downloading pandas-1.3.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.3 MB 4.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from nlb-tools==0.0.1) (1.7.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from nlb-tools==0.0.1) (1.21.6)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from nlb-tools==0.0.1) (1.0.2)\n",
            "Requirement already satisfied: h5py<4,>=2.9 in /usr/local/lib/python3.7/dist-packages (from nlb-tools==0.0.1) (3.1.0)\n",
            "Requirement already satisfied: pynwb in /usr/local/lib/python3.7/dist-packages (from nlb-tools==0.0.1) (2.1.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py<4,>=2.9->nlb-tools==0.0.1) (1.5.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas<=1.3.4,>=1.0.0->nlb-tools==0.0.1) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas<=1.3.4,>=1.0.0->nlb-tools==0.0.1) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas<=1.3.4,>=1.0.0->nlb-tools==0.0.1) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pynwb->nlb-tools==0.0.1) (57.4.0)\n",
            "Requirement already satisfied: hdmf<4,>=3.3.2 in /usr/local/lib/python3.7/dist-packages (from pynwb->nlb-tools==0.0.1) (3.3.2)\n",
            "Requirement already satisfied: ruamel.yaml<1,>=0.16 in /usr/local/lib/python3.7/dist-packages (from hdmf<4,>=3.3.2->pynwb->nlb-tools==0.0.1) (0.17.21)\n",
            "Requirement already satisfied: jsonschema<5,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from hdmf<4,>=3.3.2->pynwb->nlb-tools==0.0.1) (4.3.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema<5,>=2.6.0->hdmf<4,>=3.3.2->pynwb->nlb-tools==0.0.1) (4.12.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema<5,>=2.6.0->hdmf<4,>=3.3.2->pynwb->nlb-tools==0.0.1) (0.18.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from jsonschema<5,>=2.6.0->hdmf<4,>=3.3.2->pynwb->nlb-tools==0.0.1) (4.1.1)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema<5,>=2.6.0->hdmf<4,>=3.3.2->pynwb->nlb-tools==0.0.1) (5.9.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema<5,>=2.6.0->hdmf<4,>=3.3.2->pynwb->nlb-tools==0.0.1) (22.1.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema<5,>=2.6.0->hdmf<4,>=3.3.2->pynwb->nlb-tools==0.0.1) (3.8.1)\n",
            "Requirement already satisfied: ruamel.yaml.clib>=0.2.6 in /usr/local/lib/python3.7/dist-packages (from ruamel.yaml<1,>=0.16->hdmf<4,>=3.3.2->pynwb->nlb-tools==0.0.1) (0.2.6)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->nlb-tools==0.0.1) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->nlb-tools==0.0.1) (1.1.0)\n",
            "Building wheels for collected packages: nlb-tools\n",
            "  Building wheel for nlb-tools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nlb-tools: filename=nlb_tools-0.0.1-py3-none-any.whl size=32989 sha256=01c4ffee7ff8552d267f4456a6c9fc3e7f158519022da8c97991b5e4762749f8\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-1lsc1k4d/wheels/2a/e3/dc/2d4b1ad404062cfe0a6b190a5ea438d97d3a82a023eb5f6e8b\n",
            "Successfully built nlb-tools\n",
            "Installing collected packages: pandas, nlb-tools\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.3.5\n",
            "    Uninstalling pandas-1.3.5:\n",
            "      Successfully uninstalled pandas-1.3.5\n",
            "Successfully installed nlb-tools-0.0.1 pandas-1.3.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pandas"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'jax-lfads' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ! rm -r /content/jax-lfads\n",
        "# ! rm -r /content/computation-thru-dynamics"
      ],
      "metadata": {
        "id": "sCF0bB102h-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction"
      ],
      "metadata": {
        "id": "u6JudZRsoFNG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Recurrent neural networks (RNNs)\n",
        "\n",
        "Let us first consider the simplest, yet most important example of a non-linear discrete-time latent dynamical system: recurrent neural network. It is a go-to tool for modelling sequences in deep learning.\n",
        "\n",
        "Latent dynamics: $$h_{t} = \\sigma (V h_{t-1} + U x_t + b_h)$$\n",
        "Emission model: $$o_t = W h_t + b_o $$\n",
        "\n",
        "\n",
        "![RNN scheme (Wiki)](https://upload.wikimedia.org/wikipedia/commons/thumb/b/b5/Recurrent_neural_network_unfold.svg/2560px-Recurrent_neural_network_unfold.svg.png)\n"
      ],
      "metadata": {
        "id": "eSqJHPB9fs7o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exercise 1: implement a Vanilla RNN"
      ],
      "metadata": {
        "id": "LBCUUtvMoZ_n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from lfads_tutorial.lfads import sigmoid\n",
        "\n",
        "def vanilla_rnn_params(key, n, u, ifactor=1.0, hfactor=.001, hscale=0.0):\n",
        "  \"\"\"Generate Vanilla RNN parameters\n",
        "\n",
        "  Arguments:\n",
        "    key: random.PRNGKey for random bits\n",
        "    n: hidden state size\n",
        "    u: input size\n",
        "    ifactor: scaling factor for input weights\n",
        "    hfactor: scaling factor for hidden -> hidden weights\n",
        "    hscale: scale on h0 initial condition\n",
        "\n",
        "  Returns:\n",
        "    a dictionary of parameters\n",
        "  \"\"\"\n",
        "  key, skeys = utils.keygen(key, 3)\n",
        "  ifactor = ifactor / np.sqrt(u)\n",
        "  hfactor = hfactor / np.sqrt(n)\n",
        "\n",
        "  h0 = random.normal(next(skeys), (n,)) * hscale\n",
        "  wA = random.normal(next(skeys), (n,n)) * hfactor\n",
        "  wB = random.normal(next(skeys), (n,u)) * ifactor\n",
        "  wb = np.zeros((n,))\n",
        "\n",
        "  return {'h0' : h0,\n",
        "          'wA' : wA,\n",
        "          'wB' : wB,\n",
        "          'wb' : wb}\n",
        "\n",
        "\n",
        "def vanilla_rnn(params, h, x, nonlin=np.tanh):\n",
        "  \"\"\"Implement the Vanilla RNN equations.\n",
        "\n",
        "  Arguments:\n",
        "    params: dictionary of RNN parameters\n",
        "    h: np array of hidden state\n",
        "    x: np array of input\n",
        "\n",
        "  Returns:\n",
        "    np array of hidden state after RNN update\"\"\"\n",
        "  h_new = nonlin(np.dot(params['wA'], h) + np.dot(params['wB'], x) + params['wb'])\n",
        "  return h_new\n"
      ],
      "metadata": {
        "id": "sjJJrBIWpJOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# You must change this to the location of the computation-thru-dynamics directory.\n",
        "HOME_DIR = '/content' \n",
        "\n",
        "# sys.path.append(os.path.join(HOME_DIR,'computation-thru-dynamics'))\n",
        "sys.path.append(os.path.join(HOME_DIR,'jax-lfads'))\n",
        "import lfads_tutorial.lfads as lfads\n",
        "import lfads_tutorial.plotting as plotting\n",
        "import lfads_tutorial.utils as utils\n",
        "from lfads_tutorial.optimize import optimize_lfads, get_kl_warmup_fun\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from nlb_tools.nwb_interface import NWBDataset"
      ],
      "metadata": {
        "id": "6yb32yVUOY9i"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 3.3** Auto-encoding sequential neural data with RNN-based LFADS model "
      ],
      "metadata": {
        "id": "p1oUSJD_LtKm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "onp_rng = onp.random.RandomState(seed=0) # For CPU-based numpy randomness"
      ],
      "metadata": {
        "id": "HJfOqmLlQaLO"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make directories\n",
        "lfads_dir = '/content/lfads/'       # where to save lfads data and parameters to\n",
        "output_dir = os.path.join(lfads_dir, 'output/')\n",
        "figure_dir = os.path.join(lfads_dir, os.path.join(output_dir, 'figures/'))\n",
        "if not os.path.exists(lfads_dir):\n",
        "    os.makedirs(output_dir)\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "if not os.path.exists(figure_dir):\n",
        "    os.makedirs(figure_dir)\n",
        "\n",
        "# Load data\n",
        "\n",
        "# for simplicity, we are using NLB tools \n",
        "dataset = NWBDataset(\"/content/000138/sub-Jenkins\", \"*train\", \n",
        "                     split_heldout=False)\n",
        "# dataset = NWBDataset(\"/content/000129/sub-Indy\", \"*train\", \n",
        "#                      split_heldout=False)\n",
        "\n",
        "bin = 10 # [ms]\n",
        "dataset.resample(bin)\n",
        "\n",
        "# to view the dataset, uncomment the next line\n",
        "# dataset.data"
      ],
      "metadata": {
        "id": "786mj14UkZng"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.trial_info"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 921
        },
        "id": "s4XA3ZOWsmA7",
        "outputId": "a33e5d0c-7c72-482c-aa10-4c6faf8b51f9"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     trial_id             start_time               end_time  trial_type  \\\n",
              "0           0        0 days 00:00:00 0 days 00:00:03.341000           4   \n",
              "1           1 0 days 00:00:03.400000 0 days 00:00:06.776000           2   \n",
              "2           2 0 days 00:00:06.800000 0 days 00:00:09.661000           7   \n",
              "3           3 0 days 00:00:09.700000 0 days 00:00:12.476000          10   \n",
              "4           4 0 days 00:00:12.500000 0 days 00:00:15.106000          11   \n",
              "..        ...                    ...                    ...         ...   \n",
              "495       495 0 days 00:24:29.600000 0 days 00:24:32.636000           8   \n",
              "496       496 0 days 00:24:32.700000 0 days 00:24:35.746000          11   \n",
              "497       497 0 days 00:24:35.800000 0 days 00:24:38.801000           7   \n",
              "498       498 0 days 00:24:38.900000 0 days 00:24:41.956000           7   \n",
              "499       499        0 days 00:24:42 0 days 00:24:45.021000           6   \n",
              "\n",
              "     trial_version  maze_id  success         target_on_time  \\\n",
              "0                2       34     True 0 days 00:00:00.775000   \n",
              "1                1       32     True 0 days 00:00:04.300000   \n",
              "2                1       37     True 0 days 00:00:07.582000   \n",
              "3                0       79     True 0 days 00:00:10.600000   \n",
              "4                0       80     True 0 days 00:00:13.364000   \n",
              "..             ...      ...      ...                    ...   \n",
              "495              1       38     True 0 days 00:24:30.405000   \n",
              "496              2       80     True 0 days 00:24:33.467000   \n",
              "497              2       37     True 0 days 00:24:36.545000   \n",
              "498              1       37     True 0 days 00:24:39.613000   \n",
              "499              2       36     True 0 days 00:24:42.877000   \n",
              "\n",
              "               go_cue_time        move_onset_time   rt  delay  num_targets  \\\n",
              "0   0 days 00:00:01.773000 0 days 00:00:02.095000  322    998            3   \n",
              "1   0 days 00:00:05.198000 0 days 00:00:05.606000  408    898            1   \n",
              "2   0 days 00:00:08.114000 0 days 00:00:08.452000  338    532            1   \n",
              "3   0 days 00:00:11.015000 0 days 00:00:11.332000  317    415            1   \n",
              "4   0 days 00:00:13.695000 0 days 00:00:13.992000  297    331            1   \n",
              "..                     ...                    ...  ...    ...          ...   \n",
              "495 0 days 00:24:31.153000 0 days 00:24:31.501000  348    748            1   \n",
              "496 0 days 00:24:34.116000 0 days 00:24:34.595000  479    649            3   \n",
              "497 0 days 00:24:37.410000 0 days 00:24:37.701000  291    865            3   \n",
              "498 0 days 00:24:40.479000 0 days 00:24:40.777000  298    866            1   \n",
              "499 0 days 00:24:43.426000 0 days 00:24:43.837000  411    549            3   \n",
              "\n",
              "                               target_pos  num_barriers  \\\n",
              "0       [[123, 69], [12, 84], [-126, 15]]             6   \n",
              "1                              [[12, 84]]             6   \n",
              "2                             [[103, 83]]             9   \n",
              "3                            [[123, -81]]             0   \n",
              "4                             [[123, 71]]             0   \n",
              "..                                    ...           ...   \n",
              "495                          [[-105, 76]]             9   \n",
              "496  [[123, -81], [-130, -13], [123, 71]]             8   \n",
              "497   [[124, -79], [103, 83], [-105, 76]]             9   \n",
              "498                           [[103, 83]]             9   \n",
              "499   [[124, -79], [103, 83], [-105, 76]]             9   \n",
              "\n",
              "                                           barrier_pos  active_target  split  \n",
              "0    [[63, 48, 10, 69], [36, 51, 51, 8], [91, -78, ...              2  train  \n",
              "1    [[63, 48, 10, 69], [36, 51, 51, 8], [91, -78, ...              0  train  \n",
              "2    [[74, -102, 11, 53], [86, -44, 14, 11], [103, ...              0  train  \n",
              "3                                                   []              0  train  \n",
              "4                                                   []              0    val  \n",
              "..                                                 ...            ...    ...  \n",
              "495  [[74, -102, 11, 53], [86, -44, 14, 11], [103, ...              0  train  \n",
              "496  [[-65, -15, 14, 51], [-79, -55, 55, 6], [-103,...              2  train  \n",
              "497  [[74, -102, 11, 53], [86, -44, 14, 11], [103, ...              1    val  \n",
              "498  [[74, -102, 11, 53], [86, -44, 14, 11], [103, ...              0  train  \n",
              "499  [[74, -102, 11, 53], [86, -44, 14, 11], [103, ...              0  train  \n",
              "\n",
              "[500 rows x 18 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-72a412da-f4ed-417e-8a5a-8786ff16d8a1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>trial_id</th>\n",
              "      <th>start_time</th>\n",
              "      <th>end_time</th>\n",
              "      <th>trial_type</th>\n",
              "      <th>trial_version</th>\n",
              "      <th>maze_id</th>\n",
              "      <th>success</th>\n",
              "      <th>target_on_time</th>\n",
              "      <th>go_cue_time</th>\n",
              "      <th>move_onset_time</th>\n",
              "      <th>rt</th>\n",
              "      <th>delay</th>\n",
              "      <th>num_targets</th>\n",
              "      <th>target_pos</th>\n",
              "      <th>num_barriers</th>\n",
              "      <th>barrier_pos</th>\n",
              "      <th>active_target</th>\n",
              "      <th>split</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0 days 00:00:00</td>\n",
              "      <td>0 days 00:00:03.341000</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>34</td>\n",
              "      <td>True</td>\n",
              "      <td>0 days 00:00:00.775000</td>\n",
              "      <td>0 days 00:00:01.773000</td>\n",
              "      <td>0 days 00:00:02.095000</td>\n",
              "      <td>322</td>\n",
              "      <td>998</td>\n",
              "      <td>3</td>\n",
              "      <td>[[123, 69], [12, 84], [-126, 15]]</td>\n",
              "      <td>6</td>\n",
              "      <td>[[63, 48, 10, 69], [36, 51, 51, 8], [91, -78, ...</td>\n",
              "      <td>2</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0 days 00:00:03.400000</td>\n",
              "      <td>0 days 00:00:06.776000</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>32</td>\n",
              "      <td>True</td>\n",
              "      <td>0 days 00:00:04.300000</td>\n",
              "      <td>0 days 00:00:05.198000</td>\n",
              "      <td>0 days 00:00:05.606000</td>\n",
              "      <td>408</td>\n",
              "      <td>898</td>\n",
              "      <td>1</td>\n",
              "      <td>[[12, 84]]</td>\n",
              "      <td>6</td>\n",
              "      <td>[[63, 48, 10, 69], [36, 51, 51, 8], [91, -78, ...</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0 days 00:00:06.800000</td>\n",
              "      <td>0 days 00:00:09.661000</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>37</td>\n",
              "      <td>True</td>\n",
              "      <td>0 days 00:00:07.582000</td>\n",
              "      <td>0 days 00:00:08.114000</td>\n",
              "      <td>0 days 00:00:08.452000</td>\n",
              "      <td>338</td>\n",
              "      <td>532</td>\n",
              "      <td>1</td>\n",
              "      <td>[[103, 83]]</td>\n",
              "      <td>9</td>\n",
              "      <td>[[74, -102, 11, 53], [86, -44, 14, 11], [103, ...</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0 days 00:00:09.700000</td>\n",
              "      <td>0 days 00:00:12.476000</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "      <td>79</td>\n",
              "      <td>True</td>\n",
              "      <td>0 days 00:00:10.600000</td>\n",
              "      <td>0 days 00:00:11.015000</td>\n",
              "      <td>0 days 00:00:11.332000</td>\n",
              "      <td>317</td>\n",
              "      <td>415</td>\n",
              "      <td>1</td>\n",
              "      <td>[[123, -81]]</td>\n",
              "      <td>0</td>\n",
              "      <td>[]</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0 days 00:00:12.500000</td>\n",
              "      <td>0 days 00:00:15.106000</td>\n",
              "      <td>11</td>\n",
              "      <td>0</td>\n",
              "      <td>80</td>\n",
              "      <td>True</td>\n",
              "      <td>0 days 00:00:13.364000</td>\n",
              "      <td>0 days 00:00:13.695000</td>\n",
              "      <td>0 days 00:00:13.992000</td>\n",
              "      <td>297</td>\n",
              "      <td>331</td>\n",
              "      <td>1</td>\n",
              "      <td>[[123, 71]]</td>\n",
              "      <td>0</td>\n",
              "      <td>[]</td>\n",
              "      <td>0</td>\n",
              "      <td>val</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>495</th>\n",
              "      <td>495</td>\n",
              "      <td>0 days 00:24:29.600000</td>\n",
              "      <td>0 days 00:24:32.636000</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>38</td>\n",
              "      <td>True</td>\n",
              "      <td>0 days 00:24:30.405000</td>\n",
              "      <td>0 days 00:24:31.153000</td>\n",
              "      <td>0 days 00:24:31.501000</td>\n",
              "      <td>348</td>\n",
              "      <td>748</td>\n",
              "      <td>1</td>\n",
              "      <td>[[-105, 76]]</td>\n",
              "      <td>9</td>\n",
              "      <td>[[74, -102, 11, 53], [86, -44, 14, 11], [103, ...</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>496</th>\n",
              "      <td>496</td>\n",
              "      <td>0 days 00:24:32.700000</td>\n",
              "      <td>0 days 00:24:35.746000</td>\n",
              "      <td>11</td>\n",
              "      <td>2</td>\n",
              "      <td>80</td>\n",
              "      <td>True</td>\n",
              "      <td>0 days 00:24:33.467000</td>\n",
              "      <td>0 days 00:24:34.116000</td>\n",
              "      <td>0 days 00:24:34.595000</td>\n",
              "      <td>479</td>\n",
              "      <td>649</td>\n",
              "      <td>3</td>\n",
              "      <td>[[123, -81], [-130, -13], [123, 71]]</td>\n",
              "      <td>8</td>\n",
              "      <td>[[-65, -15, 14, 51], [-79, -55, 55, 6], [-103,...</td>\n",
              "      <td>2</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>497</th>\n",
              "      <td>497</td>\n",
              "      <td>0 days 00:24:35.800000</td>\n",
              "      <td>0 days 00:24:38.801000</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>37</td>\n",
              "      <td>True</td>\n",
              "      <td>0 days 00:24:36.545000</td>\n",
              "      <td>0 days 00:24:37.410000</td>\n",
              "      <td>0 days 00:24:37.701000</td>\n",
              "      <td>291</td>\n",
              "      <td>865</td>\n",
              "      <td>3</td>\n",
              "      <td>[[124, -79], [103, 83], [-105, 76]]</td>\n",
              "      <td>9</td>\n",
              "      <td>[[74, -102, 11, 53], [86, -44, 14, 11], [103, ...</td>\n",
              "      <td>1</td>\n",
              "      <td>val</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>498</th>\n",
              "      <td>498</td>\n",
              "      <td>0 days 00:24:38.900000</td>\n",
              "      <td>0 days 00:24:41.956000</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>37</td>\n",
              "      <td>True</td>\n",
              "      <td>0 days 00:24:39.613000</td>\n",
              "      <td>0 days 00:24:40.479000</td>\n",
              "      <td>0 days 00:24:40.777000</td>\n",
              "      <td>298</td>\n",
              "      <td>866</td>\n",
              "      <td>1</td>\n",
              "      <td>[[103, 83]]</td>\n",
              "      <td>9</td>\n",
              "      <td>[[74, -102, 11, 53], [86, -44, 14, 11], [103, ...</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>499</th>\n",
              "      <td>499</td>\n",
              "      <td>0 days 00:24:42</td>\n",
              "      <td>0 days 00:24:45.021000</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>36</td>\n",
              "      <td>True</td>\n",
              "      <td>0 days 00:24:42.877000</td>\n",
              "      <td>0 days 00:24:43.426000</td>\n",
              "      <td>0 days 00:24:43.837000</td>\n",
              "      <td>411</td>\n",
              "      <td>549</td>\n",
              "      <td>3</td>\n",
              "      <td>[[124, -79], [103, 83], [-105, 76]]</td>\n",
              "      <td>9</td>\n",
              "      <td>[[74, -102, 11, 53], [86, -44, 14, 11], [103, ...</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>500 rows × 18 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-72a412da-f4ed-417e-8a5a-8786ff16d8a1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-72a412da-f4ed-417e-8a5a-8786ff16d8a1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-72a412da-f4ed-417e-8a5a-8786ff16d8a1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_key = 'spikes'\n",
        "output_key = 'hand_vel'\n",
        "\n",
        "# Extract neural data and lagged hand velocity\n",
        "trial_length = 1300//bin\n",
        "trial_data = dataset.make_trial_data(align_field='target_on_time', \n",
        "                                     align_range=(0, 0+bin*trial_length))\n",
        "\n",
        "print(trial_data['trial_id'].to_numpy().reshape((-1,trial_length)))\n",
        "inputs = trial_data[input_key].to_numpy() \n",
        "outputs = trial_data[output_key].to_numpy()\n",
        "\n",
        "input_size = inputs.shape[-1]\n",
        "output_size = outputs.shape[-1]\n",
        "\n",
        "print(input_size,output_size)\n",
        "\n",
        "inputs = inputs.reshape((-1,trial_length,input_size)) # [trials x time x inputs]\n",
        "outputs = outputs.reshape((-1,trial_length,output_size)) # [trials x time x outputs]\n",
        " \n",
        "data_bxtxn = inputs\n",
        "data_bxtxn.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aCF02Srbm4ne",
        "outputId": "45da4b9a-e7fe-44b3-d59d-c2e5653507a5"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  0   0   0 ...   0   0   0]\n",
            " [  1   1   1 ...   1   1   1]\n",
            " [  2   2   2 ...   2   2   2]\n",
            " ...\n",
            " [497 497 497 ... 497 497 497]\n",
            " [498 498 498 ... 498 498 498]\n",
            " [499 499 499 ... 499 499 499]]\n",
            "162 2\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(500, 130, 162)"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(outputs.std(0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "id": "yMapWLsOm6oB",
        "outputId": "8d33a26d-3340-43f8-80e2-447b978d135b"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f933238d9d0>,\n",
              " <matplotlib.lines.Line2D at 0x7f933238dc90>]"
            ]
          },
          "metadata": {},
          "execution_count": 121
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_dt = 10.0        # define our dt in a physiological range [ms]\n",
        "\n",
        "train_fraction = 0.9      # Train with 90% of the synthetic data\n",
        "\n",
        "nexamples, ntimesteps, data_dim = data_bxtxn.shape\n",
        "\n",
        "train_data, eval_data = utils.split_data(data_bxtxn,\n",
        "                                         train_fraction=train_fraction)\n",
        "eval_data_offset = int(train_fraction * data_bxtxn.shape[0])"
      ],
      "metadata": {
        "id": "lBEJMxBylGo5"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hk6vMIBnyVbW",
        "outputId": "832f8ce9-aebe-4d0a-b0cb-f76aec1d0100"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50, 130, 162)"
            ]
          },
          "metadata": {},
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "ncomponents = 100\n",
        "full_pca = sklearn.decomposition.PCA(ncomponents)\n",
        "full_pca.fit(onp.reshape(data_bxtxn, [-1, data_dim]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6yCeoZtnFti",
        "outputId": "98b4e2c1-deae-4c02-df3d-7466cc5534e0"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PCA(n_components=100)"
            ]
          },
          "metadata": {},
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.stem(full_pca.explained_variance_)\n",
        "plt.title('Classic exponential spectrum');"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "zIokKxsNnzCZ",
        "outputId": "2b50ec6d-1e31-4b5b-a133-f28350d92f3c"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: UserWarning:\n",
            "\n",
            "In Matplotlib 3.3 individual lines on a stem plot will be added as a LineCollection instead of individual lines. This significantly improves the performance of a stem plot. To remove this warning and switch to the new behaviour, set the \"use_line_collection\" keyword argument to True.\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import h5py\n",
        "\n",
        "# data = h5py.File(f'neural_BLAD.h5', 'r')\n",
        "# train_input = Tensor(data['train_data'][:]) #(num_trials, len_trial, num_neurons)\n",
        "# valid_input = Tensor(data['valid_data'][:])\n",
        "# train_output = Tensor(data['train_full_behavior'][:]) #(num_trials, len_trial, num_neurons)\n",
        "# valid_output = Tensor(data['valid_full_behavior'][:])\n",
        "\n",
        "# input_size = train_input.shape[-1]\n",
        "# output_size = train_output.shape[-1]\n",
        "\n",
        "# # print(data.keys())"
      ],
      "metadata": {
        "id": "jXjsv7IwtUlP"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LFADS Hyper parameters\n",
        "data_dim = train_data.shape[2]  # input to lfads should have dimensions:\n",
        "ntimesteps = train_data.shape[1] #   (batch_size x ntimesteps x data_dim)\n",
        "batch_size = 200      # batch size during optimization\n",
        "\n",
        "# LFADS architecture - The size of the numbers is rather arbitrary, \n",
        "# but relatively small because we know the integrator RNN isn't too high \n",
        "# dimensional in its activity.\n",
        "enc_dim = 64         # encoder dim\n",
        "con_dim = 0          # controller dim\n",
        "ii_dim = 0           # inferred input dim\n",
        "gen_dim = 64         # generator dim, should be large enough\n",
        "factors_dim = 8      # factors dim, should be large enough to capture most variance of dynamics\n",
        "\n",
        "# Numerical stability\n",
        "var_min = 0.001 # Minimal variance any gaussian can become.\n",
        "\n",
        "# Optimization HPs that percolates into model\n",
        "l2reg = 0.00002\n",
        "\n",
        "# Initial state prior parameters\n",
        "# the mean is set to zero in the code\n",
        "ic_prior_var = 0.1 # this is $\\sigma^2_p$ in above paragraph\n",
        "\n",
        "# Inferred input autoregressive prior parameters\n",
        "ar_mean = 0.0                 # process mean\n",
        "ar_autocorrelation_tau = 1.0  # seconds, how correlated each time point is, related to $\\phi$ above.\n",
        "ar_noise_variance = 0.1       # noise variance\n",
        "\n",
        "lfads_hps = {'data_dim' : data_dim, 'ntimesteps' : ntimesteps,\n",
        "             'enc_dim' : enc_dim, 'con_dim' : con_dim, 'var_min' : var_min,\n",
        "             'ic_prior_var' : ic_prior_var, 'ar_mean' : ar_mean,\n",
        "             'ar_autocorrelation_tau' : ar_autocorrelation_tau,\n",
        "             'ar_noise_variance' : ar_noise_variance,\n",
        "             'ii_dim' : ii_dim, 'gen_dim' : gen_dim,\n",
        "             'factors_dim' : factors_dim,\n",
        "             'l2reg' : l2reg,\n",
        "             'batch_size' : batch_size}\n",
        "\n",
        "num_batches = 20000         # how many batches do we train\n",
        "print_every = 100            # give information every so often\n",
        "\n",
        "# Learning rate HPs\n",
        "step_size = 0.01            # initial learning rate\n",
        "decay_factor = 0.9998       # learning rate decay param\n",
        "decay_steps = 1             # learning rate decay param\n",
        "\n",
        "# Regularization HPs\n",
        "keep_rate = 0.95             # dropout keep rate during training\n",
        "\n",
        "# Numerical stability HPs\n",
        "max_grad_norm = 1.0        # gradient clipping above this value\n",
        "\n",
        "# The fact that the start and end values are required to be floats is something I need to fix.\n",
        "kl_warmup_start = 0.0 # batch number to start KL warm-up, explicitly float\n",
        "kl_warmup_end = 500.0  # batch number to be finished with KL warm-up, explicitly float\n",
        "kl_min = 0.1 # The minimum KL value, non-zero to make sure KL doesn't grow crazy before kicking in.\n",
        "kl_max = 1.\n",
        "\n",
        "lfads_opt_hps = {'num_batches' : num_batches, 'step_size' : step_size,\n",
        "                 'decay_steps' : decay_steps, 'decay_factor' : decay_factor,\n",
        "                 'kl_min' : kl_min, 'kl_max' : kl_max, 'kl_warmup_start' : kl_warmup_start,\n",
        "                 'kl_warmup_end' : kl_warmup_end, 'keep_rate' : keep_rate,\n",
        "                 'max_grad_norm' : max_grad_norm, 'print_every' : print_every,\n",
        "                 'adam_b1' : 0.9, 'adam_b2' : 0.999, 'adam_eps' : 1e-4}\n",
        "\n",
        "class hashabledict(dict):\n",
        "    def __hash__(self):\n",
        "        return hash(tuple(sorted(self.items())))\n",
        "\n",
        "lfads_hps = hashabledict(lfads_hps)\n",
        "lfads_opt_hps = hashabledict(lfads_opt_hps)\n",
        "\n",
        "assert num_batches >= print_every and num_batches % print_every == 0"
      ],
      "metadata": {
        "id": "nPH3GH-Dnkmd"
      },
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "ZYAnnEDkpBl3"
      },
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize parameters for LFADS\n",
        "from functools import partial\n",
        "nonlin = lambda x: np.tanh(x)\n",
        "# model = partial(vanilla_rnn, **{'nonlin': nonlin})\n",
        "# model_params = vanilla_rnn_params\n",
        "\n",
        "# to get the results line in Nature Methods\n",
        "model = lfads.gru\n",
        "model_params = lfads.gru_params\n",
        "\n",
        "key = random.PRNGKey(onp.random.randint(0, utils.MAX_SEED_INT))\n",
        "init_params = lfads.lfads_params(key, lfads_hps, gen_rnn_params=model_params)"
      ],
      "metadata": {
        "id": "hqWX6Kl4oGCq"
      },
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "key = random.PRNGKey(onp.random.randint(0, utils.MAX_SEED_INT))\n",
        "trained_params, opt_details = \\\n",
        "    optimize_lfads(key, init_params, lfads_hps, lfads_opt_hps,\\\n",
        "                   train_data, eval_data, gen=model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ruJdYukooPT",
        "outputId": "7eb7d0b1-17f4-47c7-8843-4b438536921c"
      },
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batches 1-100 in 36.05 sec, Step size: 0.00980\n",
            "    Training losses 3464 = NLL 3463 + KL IC 5,0 + KL II 0,0 + L2 0.03\n",
            "        Eval losses 3424 = NLL 3423 + KL IC 4,0 + KL II 0,0 + L2 0.03\n",
            "Batches 101-200 in 7.16 sec, Step size: 0.00961\n",
            "    Training losses 3463 = NLL 3462 + KL IC 3,1 + KL II 0,0 + L2 0.04\n",
            "        Eval losses 3416 = NLL 3415 + KL IC 3,1 + KL II 0,0 + L2 0.04\n",
            "Batches 201-300 in 7.15 sec, Step size: 0.00942\n",
            "    Training losses 3463 = NLL 3462 + KL IC 3,1 + KL II 0,0 + L2 0.04\n",
            "        Eval losses 3399 = NLL 3397 + KL IC 2,1 + KL II 0,0 + L2 0.04\n",
            "Batches 301-400 in 7.14 sec, Step size: 0.00923\n",
            "    Training losses 3431 = NLL 3430 + KL IC 2,1 + KL II 0,0 + L2 0.05\n",
            "        Eval losses 3400 = NLL 3399 + KL IC 2,1 + KL II 0,0 + L2 0.05\n",
            "Batches 401-500 in 7.15 sec, Step size: 0.00905\n",
            "    Training losses 3422 = NLL 3420 + KL IC 2,2 + KL II 0,0 + L2 0.05\n",
            "        Eval losses 3397 = NLL 3396 + KL IC 2,1 + KL II 0,0 + L2 0.05\n",
            "Batches 501-600 in 7.15 sec, Step size: 0.00887\n",
            "    Training losses 3469 = NLL 3467 + KL IC 2,2 + KL II 0,0 + L2 0.05\n",
            "        Eval losses 3401 = NLL 3400 + KL IC 2,2 + KL II 0,0 + L2 0.05\n",
            "Batches 601-700 in 7.14 sec, Step size: 0.00869\n",
            "    Training losses 3418 = NLL 3416 + KL IC 2,2 + KL II 0,0 + L2 0.05\n",
            "        Eval losses 3417 = NLL 3416 + KL IC 2,2 + KL II 0,0 + L2 0.05\n",
            "Batches 701-800 in 7.14 sec, Step size: 0.00852\n",
            "    Training losses 3433 = NLL 3431 + KL IC 2,2 + KL II 0,0 + L2 0.06\n",
            "        Eval losses 3400 = NLL 3398 + KL IC 2,2 + KL II 0,0 + L2 0.06\n",
            "Batches 801-900 in 7.13 sec, Step size: 0.00835\n",
            "    Training losses 3451 = NLL 3449 + KL IC 2,2 + KL II 0,0 + L2 0.06\n",
            "        Eval losses 3399 = NLL 3397 + KL IC 2,2 + KL II 0,0 + L2 0.06\n",
            "Batches 901-1000 in 7.15 sec, Step size: 0.00819\n",
            "    Training losses 3425 = NLL 3423 + KL IC 2,2 + KL II 0,0 + L2 0.06\n",
            "        Eval losses 3377 = NLL 3375 + KL IC 2,2 + KL II 0,0 + L2 0.06\n",
            "Batches 1001-1100 in 7.14 sec, Step size: 0.00803\n",
            "    Training losses 3408 = NLL 3405 + KL IC 2,2 + KL II 0,0 + L2 0.07\n",
            "        Eval losses 3408 = NLL 3406 + KL IC 2,2 + KL II 0,0 + L2 0.07\n",
            "Batches 1101-1200 in 7.14 sec, Step size: 0.00787\n",
            "    Training losses 3445 = NLL 3443 + KL IC 2,2 + KL II 0,0 + L2 0.07\n",
            "        Eval losses 3383 = NLL 3381 + KL IC 2,2 + KL II 0,0 + L2 0.07\n",
            "Batches 1201-1300 in 7.16 sec, Step size: 0.00771\n",
            "    Training losses 3404 = NLL 3401 + KL IC 2,2 + KL II 0,0 + L2 0.07\n",
            "        Eval losses 3390 = NLL 3388 + KL IC 2,2 + KL II 0,0 + L2 0.07\n",
            "Batches 1301-1400 in 7.15 sec, Step size: 0.00756\n",
            "    Training losses 3449 = NLL 3446 + KL IC 2,2 + KL II 0,0 + L2 0.07\n",
            "        Eval losses 3397 = NLL 3395 + KL IC 2,2 + KL II 0,0 + L2 0.07\n",
            "Batches 1401-1500 in 7.14 sec, Step size: 0.00741\n",
            "    Training losses 3412 = NLL 3409 + KL IC 3,3 + KL II 0,0 + L2 0.07\n",
            "        Eval losses 3377 = NLL 3374 + KL IC 2,2 + KL II 0,0 + L2 0.07\n",
            "Batches 1501-1600 in 7.15 sec, Step size: 0.00726\n",
            "    Training losses 3419 = NLL 3417 + KL IC 2,2 + KL II 0,0 + L2 0.08\n",
            "        Eval losses 3377 = NLL 3375 + KL IC 2,2 + KL II 0,0 + L2 0.08\n",
            "Batches 1601-1700 in 7.15 sec, Step size: 0.00712\n",
            "    Training losses 3420 = NLL 3417 + KL IC 2,2 + KL II 0,0 + L2 0.08\n",
            "        Eval losses 3386 = NLL 3384 + KL IC 2,2 + KL II 0,0 + L2 0.08\n",
            "Batches 1701-1800 in 7.42 sec, Step size: 0.00698\n",
            "    Training losses 3414 = NLL 3412 + KL IC 2,2 + KL II 0,0 + L2 0.08\n",
            "        Eval losses 3409 = NLL 3407 + KL IC 2,2 + KL II 0,0 + L2 0.08\n",
            "Batches 1801-1900 in 7.15 sec, Step size: 0.00684\n",
            "    Training losses 3429 = NLL 3426 + KL IC 3,3 + KL II 0,0 + L2 0.08\n",
            "        Eval losses 3344 = NLL 3342 + KL IC 2,2 + KL II 0,0 + L2 0.08\n",
            "Batches 1901-2000 in 7.15 sec, Step size: 0.00670\n",
            "    Training losses 3458 = NLL 3456 + KL IC 2,2 + KL II 0,0 + L2 0.08\n",
            "        Eval losses 3378 = NLL 3376 + KL IC 2,2 + KL II 0,0 + L2 0.08\n",
            "Batches 2001-2100 in 7.16 sec, Step size: 0.00657\n",
            "    Training losses 3375 = NLL 3373 + KL IC 3,3 + KL II 0,0 + L2 0.08\n",
            "        Eval losses 3371 = NLL 3369 + KL IC 2,2 + KL II 0,0 + L2 0.08\n",
            "Batches 2101-2200 in 7.15 sec, Step size: 0.00644\n",
            "    Training losses 3405 = NLL 3402 + KL IC 3,3 + KL II 0,0 + L2 0.08\n",
            "        Eval losses 3376 = NLL 3374 + KL IC 2,2 + KL II 0,0 + L2 0.08\n",
            "Batches 2201-2300 in 7.14 sec, Step size: 0.00631\n",
            "    Training losses 3404 = NLL 3401 + KL IC 2,2 + KL II 0,0 + L2 0.09\n",
            "        Eval losses 3347 = NLL 3345 + KL IC 2,2 + KL II 0,0 + L2 0.09\n",
            "Batches 2301-2400 in 7.12 sec, Step size: 0.00619\n",
            "    Training losses 3423 = NLL 3421 + KL IC 3,3 + KL II 0,0 + L2 0.09\n",
            "        Eval losses 3358 = NLL 3355 + KL IC 2,2 + KL II 0,0 + L2 0.09\n",
            "Batches 2401-2500 in 7.11 sec, Step size: 0.00607\n",
            "    Training losses 3418 = NLL 3416 + KL IC 3,3 + KL II 0,0 + L2 0.09\n",
            "        Eval losses 3368 = NLL 3366 + KL IC 2,2 + KL II 0,0 + L2 0.09\n",
            "Batches 2501-2600 in 7.12 sec, Step size: 0.00594\n",
            "    Training losses 3419 = NLL 3416 + KL IC 3,3 + KL II 0,0 + L2 0.09\n",
            "        Eval losses 3349 = NLL 3346 + KL IC 2,2 + KL II 0,0 + L2 0.09\n",
            "Batches 2601-2700 in 7.15 sec, Step size: 0.00583\n",
            "    Training losses 3404 = NLL 3401 + KL IC 3,3 + KL II 0,0 + L2 0.09\n",
            "        Eval losses 3361 = NLL 3359 + KL IC 2,2 + KL II 0,0 + L2 0.09\n",
            "Batches 2701-2800 in 7.13 sec, Step size: 0.00571\n",
            "    Training losses 3395 = NLL 3392 + KL IC 3,3 + KL II 0,0 + L2 0.09\n",
            "        Eval losses 3347 = NLL 3345 + KL IC 2,2 + KL II 0,0 + L2 0.09\n",
            "Batches 2801-2900 in 7.12 sec, Step size: 0.00560\n",
            "    Training losses 3418 = NLL 3415 + KL IC 3,3 + KL II 0,0 + L2 0.10\n",
            "        Eval losses 3344 = NLL 3341 + KL IC 2,2 + KL II 0,0 + L2 0.10\n",
            "Batches 2901-3000 in 7.14 sec, Step size: 0.00549\n",
            "    Training losses 3405 = NLL 3403 + KL IC 2,2 + KL II 0,0 + L2 0.10\n",
            "        Eval losses 3342 = NLL 3340 + KL IC 2,2 + KL II 0,0 + L2 0.10\n",
            "Batches 3001-3100 in 7.15 sec, Step size: 0.00538\n",
            "    Training losses 3378 = NLL 3375 + KL IC 3,3 + KL II 0,0 + L2 0.10\n",
            "        Eval losses 3371 = NLL 3369 + KL IC 2,2 + KL II 0,0 + L2 0.10\n",
            "Batches 3101-3200 in 7.15 sec, Step size: 0.00527\n",
            "    Training losses 3377 = NLL 3374 + KL IC 3,3 + KL II 0,0 + L2 0.10\n",
            "        Eval losses 3339 = NLL 3336 + KL IC 3,3 + KL II 0,0 + L2 0.10\n",
            "Batches 3201-3300 in 7.14 sec, Step size: 0.00517\n",
            "    Training losses 3365 = NLL 3363 + KL IC 3,3 + KL II 0,0 + L2 0.10\n",
            "        Eval losses 3358 = NLL 3356 + KL IC 2,2 + KL II 0,0 + L2 0.10\n",
            "Batches 3301-3400 in 7.15 sec, Step size: 0.00507\n",
            "    Training losses 3422 = NLL 3419 + KL IC 3,3 + KL II 0,0 + L2 0.10\n",
            "        Eval losses 3336 = NLL 3333 + KL IC 3,3 + KL II 0,0 + L2 0.10\n",
            "Batches 3401-3500 in 7.15 sec, Step size: 0.00497\n",
            "    Training losses 3415 = NLL 3412 + KL IC 3,3 + KL II 0,0 + L2 0.10\n",
            "        Eval losses 3343 = NLL 3340 + KL IC 3,3 + KL II 0,0 + L2 0.10\n",
            "Batches 3501-3600 in 7.45 sec, Step size: 0.00487\n",
            "    Training losses 3385 = NLL 3382 + KL IC 3,3 + KL II 0,0 + L2 0.10\n",
            "        Eval losses 3330 = NLL 3327 + KL IC 2,2 + KL II 0,0 + L2 0.10\n",
            "Batches 3601-3700 in 7.14 sec, Step size: 0.00477\n",
            "    Training losses 3404 = NLL 3401 + KL IC 3,3 + KL II 0,0 + L2 0.11\n",
            "        Eval losses 3354 = NLL 3352 + KL IC 2,2 + KL II 0,0 + L2 0.11\n",
            "Batches 3701-3800 in 7.14 sec, Step size: 0.00468\n",
            "    Training losses 3387 = NLL 3384 + KL IC 3,3 + KL II 0,0 + L2 0.11\n",
            "        Eval losses 3350 = NLL 3347 + KL IC 2,2 + KL II 0,0 + L2 0.11\n",
            "Batches 3801-3900 in 7.13 sec, Step size: 0.00458\n",
            "    Training losses 3411 = NLL 3408 + KL IC 3,3 + KL II 0,0 + L2 0.11\n",
            "        Eval losses 3325 = NLL 3322 + KL IC 2,2 + KL II 0,0 + L2 0.11\n",
            "Batches 3901-4000 in 7.14 sec, Step size: 0.00449\n",
            "    Training losses 3397 = NLL 3394 + KL IC 3,3 + KL II 0,0 + L2 0.11\n",
            "        Eval losses 3367 = NLL 3364 + KL IC 3,3 + KL II 0,0 + L2 0.11\n",
            "Batches 4001-4100 in 7.13 sec, Step size: 0.00440\n",
            "    Training losses 3388 = NLL 3385 + KL IC 3,3 + KL II 0,0 + L2 0.11\n",
            "        Eval losses 3357 = NLL 3354 + KL IC 2,2 + KL II 0,0 + L2 0.11\n",
            "Batches 4101-4200 in 7.15 sec, Step size: 0.00432\n",
            "    Training losses 3412 = NLL 3410 + KL IC 3,3 + KL II 0,0 + L2 0.11\n",
            "        Eval losses 3355 = NLL 3353 + KL IC 2,2 + KL II 0,0 + L2 0.11\n",
            "Batches 4201-4300 in 7.14 sec, Step size: 0.00423\n",
            "    Training losses 3384 = NLL 3381 + KL IC 3,3 + KL II 0,0 + L2 0.11\n",
            "        Eval losses 3368 = NLL 3365 + KL IC 3,3 + KL II 0,0 + L2 0.11\n",
            "Batches 4301-4400 in 7.16 sec, Step size: 0.00415\n",
            "    Training losses 3390 = NLL 3387 + KL IC 3,3 + KL II 0,0 + L2 0.11\n",
            "        Eval losses 3328 = NLL 3325 + KL IC 2,2 + KL II 0,0 + L2 0.11\n",
            "Batches 4401-4500 in 7.15 sec, Step size: 0.00407\n",
            "    Training losses 3389 = NLL 3386 + KL IC 3,3 + KL II 0,0 + L2 0.11\n",
            "        Eval losses 3329 = NLL 3326 + KL IC 3,3 + KL II 0,0 + L2 0.11\n",
            "Batches 4501-4600 in 7.14 sec, Step size: 0.00398\n",
            "    Training losses 3396 = NLL 3393 + KL IC 3,3 + KL II 0,0 + L2 0.11\n",
            "        Eval losses 3371 = NLL 3368 + KL IC 3,3 + KL II 0,0 + L2 0.11\n",
            "Batches 4601-4700 in 7.16 sec, Step size: 0.00391\n",
            "    Training losses 3395 = NLL 3392 + KL IC 3,3 + KL II 0,0 + L2 0.11\n",
            "        Eval losses 3344 = NLL 3341 + KL IC 3,3 + KL II 0,0 + L2 0.11\n",
            "Batches 4701-4800 in 7.13 sec, Step size: 0.00383\n",
            "    Training losses 3418 = NLL 3414 + KL IC 3,3 + KL II 0,0 + L2 0.12\n",
            "        Eval losses 3340 = NLL 3337 + KL IC 3,3 + KL II 0,0 + L2 0.12\n",
            "Batches 4801-4900 in 7.13 sec, Step size: 0.00375\n",
            "    Training losses 3357 = NLL 3353 + KL IC 3,3 + KL II 0,0 + L2 0.12\n",
            "        Eval losses 3332 = NLL 3329 + KL IC 3,3 + KL II 0,0 + L2 0.12\n",
            "Batches 4901-5000 in 7.15 sec, Step size: 0.00368\n",
            "    Training losses 3391 = NLL 3387 + KL IC 3,3 + KL II 0,0 + L2 0.12\n",
            "        Eval losses 3348 = NLL 3345 + KL IC 3,3 + KL II 0,0 + L2 0.12\n",
            "Batches 5001-5100 in 7.14 sec, Step size: 0.00361\n",
            "    Training losses 3404 = NLL 3401 + KL IC 3,3 + KL II 0,0 + L2 0.12\n",
            "        Eval losses 3346 = NLL 3343 + KL IC 3,3 + KL II 0,0 + L2 0.12\n",
            "Batches 5101-5200 in 7.14 sec, Step size: 0.00353\n",
            "    Training losses 3389 = NLL 3386 + KL IC 3,3 + KL II 0,0 + L2 0.12\n",
            "        Eval losses 3379 = NLL 3375 + KL IC 3,3 + KL II 0,0 + L2 0.12\n",
            "Batches 5201-5300 in 7.15 sec, Step size: 0.00346\n",
            "    Training losses 3363 = NLL 3359 + KL IC 3,3 + KL II 0,0 + L2 0.12\n",
            "        Eval losses 3340 = NLL 3337 + KL IC 3,3 + KL II 0,0 + L2 0.12\n",
            "Batches 5301-5400 in 7.20 sec, Step size: 0.00340\n",
            "    Training losses 3407 = NLL 3403 + KL IC 4,4 + KL II 0,0 + L2 0.12\n",
            "        Eval losses 3348 = NLL 3344 + KL IC 4,4 + KL II 0,0 + L2 0.12\n",
            "Batches 5401-5500 in 7.31 sec, Step size: 0.00333\n",
            "    Training losses 3424 = NLL 3420 + KL IC 4,4 + KL II 0,0 + L2 0.12\n",
            "        Eval losses 3314 = NLL 3311 + KL IC 3,3 + KL II 0,0 + L2 0.12\n",
            "Batches 5501-5600 in 7.12 sec, Step size: 0.00326\n",
            "    Training losses 3369 = NLL 3365 + KL IC 4,4 + KL II 0,0 + L2 0.12\n",
            "        Eval losses 3336 = NLL 3332 + KL IC 3,3 + KL II 0,0 + L2 0.12\n",
            "Batches 5601-5700 in 7.11 sec, Step size: 0.00320\n",
            "    Training losses 3378 = NLL 3374 + KL IC 4,4 + KL II 0,0 + L2 0.12\n",
            "        Eval losses 3329 = NLL 3326 + KL IC 3,3 + KL II 0,0 + L2 0.12\n",
            "Batches 5701-5800 in 7.14 sec, Step size: 0.00313\n",
            "    Training losses 3393 = NLL 3390 + KL IC 4,4 + KL II 0,0 + L2 0.13\n",
            "        Eval losses 3333 = NLL 3330 + KL IC 3,3 + KL II 0,0 + L2 0.13\n",
            "Batches 5801-5900 in 7.12 sec, Step size: 0.00307\n",
            "    Training losses 3377 = NLL 3373 + KL IC 4,4 + KL II 0,0 + L2 0.13\n",
            "        Eval losses 3335 = NLL 3331 + KL IC 4,4 + KL II 0,0 + L2 0.13\n",
            "Batches 5901-6000 in 7.12 sec, Step size: 0.00301\n",
            "    Training losses 3359 = NLL 3355 + KL IC 4,4 + KL II 0,0 + L2 0.13\n",
            "        Eval losses 3317 = NLL 3313 + KL IC 3,3 + KL II 0,0 + L2 0.13\n",
            "Batches 6001-6100 in 7.11 sec, Step size: 0.00295\n",
            "    Training losses 3412 = NLL 3409 + KL IC 3,3 + KL II 0,0 + L2 0.13\n",
            "        Eval losses 3319 = NLL 3316 + KL IC 3,3 + KL II 0,0 + L2 0.13\n",
            "Batches 6101-6200 in 7.16 sec, Step size: 0.00289\n",
            "    Training losses 3383 = NLL 3379 + KL IC 4,4 + KL II 0,0 + L2 0.13\n",
            "        Eval losses 3323 = NLL 3320 + KL IC 4,4 + KL II 0,0 + L2 0.13\n",
            "Batches 6201-6300 in 7.15 sec, Step size: 0.00284\n",
            "    Training losses 3346 = NLL 3343 + KL IC 4,4 + KL II 0,0 + L2 0.13\n",
            "        Eval losses 3295 = NLL 3291 + KL IC 3,3 + KL II 0,0 + L2 0.13\n",
            "Batches 6301-6400 in 7.14 sec, Step size: 0.00278\n",
            "    Training losses 3336 = NLL 3333 + KL IC 4,4 + KL II 0,0 + L2 0.13\n",
            "        Eval losses 3337 = NLL 3334 + KL IC 3,3 + KL II 0,0 + L2 0.13\n",
            "Batches 6401-6500 in 7.15 sec, Step size: 0.00272\n",
            "    Training losses 3395 = NLL 3391 + KL IC 4,4 + KL II 0,0 + L2 0.13\n",
            "        Eval losses 3308 = NLL 3304 + KL IC 4,4 + KL II 0,0 + L2 0.13\n",
            "Batches 6501-6600 in 7.15 sec, Step size: 0.00267\n",
            "    Training losses 3362 = NLL 3359 + KL IC 4,4 + KL II 0,0 + L2 0.13\n",
            "        Eval losses 3315 = NLL 3311 + KL IC 3,3 + KL II 0,0 + L2 0.13\n",
            "Batches 6601-6700 in 7.13 sec, Step size: 0.00262\n",
            "    Training losses 3409 = NLL 3405 + KL IC 4,4 + KL II 0,0 + L2 0.13\n",
            "        Eval losses 3315 = NLL 3311 + KL IC 4,4 + KL II 0,0 + L2 0.13\n",
            "Batches 6701-6800 in 7.15 sec, Step size: 0.00257\n",
            "    Training losses 3384 = NLL 3380 + KL IC 4,4 + KL II 0,0 + L2 0.13\n",
            "        Eval losses 3326 = NLL 3322 + KL IC 4,4 + KL II 0,0 + L2 0.13\n",
            "Batches 6801-6900 in 7.14 sec, Step size: 0.00252\n",
            "    Training losses 3396 = NLL 3392 + KL IC 4,4 + KL II 0,0 + L2 0.13\n",
            "        Eval losses 3324 = NLL 3320 + KL IC 3,3 + KL II 0,0 + L2 0.13\n",
            "Batches 6901-7000 in 7.14 sec, Step size: 0.00247\n",
            "    Training losses 3348 = NLL 3344 + KL IC 4,4 + KL II 0,0 + L2 0.13\n",
            "        Eval losses 3285 = NLL 3281 + KL IC 4,4 + KL II 0,0 + L2 0.13\n",
            "Batches 7001-7100 in 7.15 sec, Step size: 0.00242\n",
            "    Training losses 3368 = NLL 3365 + KL IC 4,4 + KL II 0,0 + L2 0.14\n",
            "        Eval losses 3312 = NLL 3308 + KL IC 3,3 + KL II 0,0 + L2 0.14\n",
            "Batches 7101-7200 in 7.14 sec, Step size: 0.00237\n",
            "    Training losses 3339 = NLL 3335 + KL IC 4,4 + KL II 0,0 + L2 0.14\n",
            "        Eval losses 3334 = NLL 3331 + KL IC 3,3 + KL II 0,0 + L2 0.14\n",
            "Batches 7201-7300 in 7.40 sec, Step size: 0.00232\n",
            "    Training losses 3376 = NLL 3372 + KL IC 4,4 + KL II 0,0 + L2 0.14\n",
            "        Eval losses 3311 = NLL 3307 + KL IC 3,3 + KL II 0,0 + L2 0.14\n",
            "Batches 7301-7400 in 7.14 sec, Step size: 0.00228\n",
            "    Training losses 3395 = NLL 3391 + KL IC 4,4 + KL II 0,0 + L2 0.14\n",
            "        Eval losses 3327 = NLL 3324 + KL IC 4,4 + KL II 0,0 + L2 0.14\n",
            "Batches 7401-7500 in 7.14 sec, Step size: 0.00223\n",
            "    Training losses 3368 = NLL 3365 + KL IC 4,4 + KL II 0,0 + L2 0.14\n",
            "        Eval losses 3320 = NLL 3317 + KL IC 3,3 + KL II 0,0 + L2 0.14\n",
            "Batches 7501-7600 in 7.14 sec, Step size: 0.00219\n",
            "    Training losses 3411 = NLL 3407 + KL IC 4,4 + KL II 0,0 + L2 0.14\n",
            "        Eval losses 3328 = NLL 3324 + KL IC 3,3 + KL II 0,0 + L2 0.14\n",
            "Batches 7601-7700 in 7.14 sec, Step size: 0.00214\n",
            "    Training losses 3367 = NLL 3363 + KL IC 4,4 + KL II 0,0 + L2 0.14\n",
            "        Eval losses 3286 = NLL 3283 + KL IC 4,4 + KL II 0,0 + L2 0.14\n",
            "Batches 7701-7800 in 7.12 sec, Step size: 0.00210\n",
            "    Training losses 3346 = NLL 3342 + KL IC 4,4 + KL II 0,0 + L2 0.14\n",
            "        Eval losses 3320 = NLL 3317 + KL IC 4,4 + KL II 0,0 + L2 0.14\n",
            "Batches 7801-7900 in 7.13 sec, Step size: 0.00206\n",
            "    Training losses 3368 = NLL 3365 + KL IC 4,4 + KL II 0,0 + L2 0.14\n",
            "        Eval losses 3330 = NLL 3327 + KL IC 3,3 + KL II 0,0 + L2 0.14\n",
            "Batches 7901-8000 in 7.17 sec, Step size: 0.00202\n",
            "    Training losses 3370 = NLL 3366 + KL IC 4,4 + KL II 0,0 + L2 0.14\n",
            "        Eval losses 3310 = NLL 3306 + KL IC 4,4 + KL II 0,0 + L2 0.14\n",
            "Batches 8001-8100 in 7.14 sec, Step size: 0.00198\n",
            "    Training losses 3355 = NLL 3352 + KL IC 4,4 + KL II 0,0 + L2 0.14\n",
            "        Eval losses 3316 = NLL 3313 + KL IC 3,3 + KL II 0,0 + L2 0.14\n",
            "Batches 8101-8200 in 7.17 sec, Step size: 0.00194\n",
            "    Training losses 3373 = NLL 3368 + KL IC 4,4 + KL II 0,0 + L2 0.14\n",
            "        Eval losses 3315 = NLL 3311 + KL IC 4,4 + KL II 0,0 + L2 0.14\n",
            "Batches 8201-8300 in 7.17 sec, Step size: 0.00190\n",
            "    Training losses 3391 = NLL 3387 + KL IC 4,4 + KL II 0,0 + L2 0.14\n",
            "        Eval losses 3330 = NLL 3326 + KL IC 4,4 + KL II 0,0 + L2 0.14\n",
            "Batches 8301-8400 in 7.19 sec, Step size: 0.00186\n",
            "    Training losses 3336 = NLL 3332 + KL IC 4,4 + KL II 0,0 + L2 0.14\n",
            "        Eval losses 3323 = NLL 3320 + KL IC 4,4 + KL II 0,0 + L2 0.14\n",
            "Batches 8401-8500 in 7.15 sec, Step size: 0.00183\n",
            "    Training losses 3366 = NLL 3362 + KL IC 4,4 + KL II 0,0 + L2 0.14\n",
            "        Eval losses 3320 = NLL 3316 + KL IC 3,3 + KL II 0,0 + L2 0.14\n",
            "Batches 8501-8600 in 7.16 sec, Step size: 0.00179\n",
            "    Training losses 3348 = NLL 3344 + KL IC 4,4 + KL II 0,0 + L2 0.14\n",
            "        Eval losses 3347 = NLL 3343 + KL IC 4,4 + KL II 0,0 + L2 0.14\n",
            "Batches 8601-8700 in 7.13 sec, Step size: 0.00175\n",
            "    Training losses 3322 = NLL 3318 + KL IC 4,4 + KL II 0,0 + L2 0.14\n",
            "        Eval losses 3309 = NLL 3305 + KL IC 3,3 + KL II 0,0 + L2 0.14\n",
            "Batches 8701-8800 in 7.13 sec, Step size: 0.00172\n",
            "    Training losses 3380 = NLL 3376 + KL IC 4,4 + KL II 0,0 + L2 0.15\n",
            "        Eval losses 3321 = NLL 3317 + KL IC 3,3 + KL II 0,0 + L2 0.15\n",
            "Batches 8801-8900 in 7.13 sec, Step size: 0.00169\n",
            "    Training losses 3359 = NLL 3355 + KL IC 4,4 + KL II 0,0 + L2 0.15\n",
            "        Eval losses 3319 = NLL 3316 + KL IC 3,3 + KL II 0,0 + L2 0.15\n",
            "Batches 8901-9000 in 7.14 sec, Step size: 0.00165\n",
            "    Training losses 3359 = NLL 3355 + KL IC 4,4 + KL II 0,0 + L2 0.15\n",
            "        Eval losses 3321 = NLL 3317 + KL IC 4,4 + KL II 0,0 + L2 0.15\n",
            "Batches 9001-9100 in 7.26 sec, Step size: 0.00162\n",
            "    Training losses 3357 = NLL 3353 + KL IC 4,4 + KL II 0,0 + L2 0.15\n",
            "        Eval losses 3327 = NLL 3324 + KL IC 3,3 + KL II 0,0 + L2 0.15\n",
            "Batches 9101-9200 in 8.80 sec, Step size: 0.00159\n",
            "    Training losses 3378 = NLL 3374 + KL IC 4,4 + KL II 0,0 + L2 0.15\n",
            "        Eval losses 3312 = NLL 3308 + KL IC 3,3 + KL II 0,0 + L2 0.15\n",
            "Batches 9201-9300 in 7.15 sec, Step size: 0.00156\n",
            "    Training losses 3381 = NLL 3377 + KL IC 4,4 + KL II 0,0 + L2 0.15\n",
            "        Eval losses 3318 = NLL 3314 + KL IC 4,4 + KL II 0,0 + L2 0.15\n",
            "Batches 9301-9400 in 7.16 sec, Step size: 0.00153\n",
            "    Training losses 3349 = NLL 3345 + KL IC 4,4 + KL II 0,0 + L2 0.15\n",
            "        Eval losses 3316 = NLL 3312 + KL IC 3,3 + KL II 0,0 + L2 0.15\n",
            "Batches 9401-9500 in 7.15 sec, Step size: 0.00150\n",
            "    Training losses 3349 = NLL 3345 + KL IC 4,4 + KL II 0,0 + L2 0.15\n",
            "        Eval losses 3298 = NLL 3294 + KL IC 4,4 + KL II 0,0 + L2 0.15\n",
            "Batches 9501-9600 in 7.14 sec, Step size: 0.00147\n",
            "    Training losses 3376 = NLL 3372 + KL IC 4,4 + KL II 0,0 + L2 0.15\n",
            "        Eval losses 3313 = NLL 3309 + KL IC 4,4 + KL II 0,0 + L2 0.15\n",
            "Batches 9601-9700 in 7.14 sec, Step size: 0.00144\n",
            "    Training losses 3385 = NLL 3381 + KL IC 4,4 + KL II 0,0 + L2 0.15\n",
            "        Eval losses 3304 = NLL 3300 + KL IC 3,3 + KL II 0,0 + L2 0.15\n",
            "Batches 9701-9800 in 7.15 sec, Step size: 0.00141\n",
            "    Training losses 3354 = NLL 3350 + KL IC 4,4 + KL II 0,0 + L2 0.15\n",
            "        Eval losses 3319 = NLL 3316 + KL IC 3,3 + KL II 0,0 + L2 0.15\n",
            "Batches 9801-9900 in 7.13 sec, Step size: 0.00138\n",
            "    Training losses 3388 = NLL 3384 + KL IC 4,4 + KL II 0,0 + L2 0.15\n",
            "        Eval losses 3324 = NLL 3320 + KL IC 4,4 + KL II 0,0 + L2 0.15\n",
            "Batches 9901-10000 in 7.14 sec, Step size: 0.00135\n",
            "    Training losses 3347 = NLL 3343 + KL IC 4,4 + KL II 0,0 + L2 0.15\n",
            "        Eval losses 3325 = NLL 3321 + KL IC 4,4 + KL II 0,0 + L2 0.15\n",
            "Batches 10001-10100 in 7.14 sec, Step size: 0.00133\n",
            "    Training losses 3387 = NLL 3383 + KL IC 4,4 + KL II 0,0 + L2 0.15\n",
            "        Eval losses 3292 = NLL 3288 + KL IC 4,4 + KL II 0,0 + L2 0.15\n",
            "Batches 10101-10200 in 7.16 sec, Step size: 0.00130\n",
            "    Training losses 3342 = NLL 3338 + KL IC 4,4 + KL II 0,0 + L2 0.15\n",
            "        Eval losses 3321 = NLL 3317 + KL IC 3,3 + KL II 0,0 + L2 0.15\n",
            "Batches 10201-10300 in 7.14 sec, Step size: 0.00127\n",
            "    Training losses 3384 = NLL 3381 + KL IC 4,4 + KL II 0,0 + L2 0.15\n",
            "        Eval losses 3296 = NLL 3292 + KL IC 3,3 + KL II 0,0 + L2 0.15\n",
            "Batches 10301-10400 in 7.14 sec, Step size: 0.00125\n",
            "    Training losses 3351 = NLL 3347 + KL IC 4,4 + KL II 0,0 + L2 0.15\n",
            "        Eval losses 3315 = NLL 3311 + KL IC 4,4 + KL II 0,0 + L2 0.15\n",
            "Batches 10401-10500 in 7.15 sec, Step size: 0.00122\n",
            "    Training losses 3354 = NLL 3349 + KL IC 4,4 + KL II 0,0 + L2 0.15\n",
            "        Eval losses 3310 = NLL 3306 + KL IC 4,4 + KL II 0,0 + L2 0.15\n",
            "Batches 10501-10600 in 7.15 sec, Step size: 0.00120\n",
            "    Training losses 3308 = NLL 3304 + KL IC 4,4 + KL II 0,0 + L2 0.15\n",
            "        Eval losses 3328 = NLL 3324 + KL IC 4,4 + KL II 0,0 + L2 0.15\n",
            "Batches 10601-10700 in 7.68 sec, Step size: 0.00118\n",
            "    Training losses 3382 = NLL 3378 + KL IC 4,4 + KL II 0,0 + L2 0.15\n",
            "        Eval losses 3310 = NLL 3306 + KL IC 4,4 + KL II 0,0 + L2 0.15\n",
            "Batches 10701-10800 in 8.14 sec, Step size: 0.00115\n",
            "    Training losses 3364 = NLL 3360 + KL IC 4,4 + KL II 0,0 + L2 0.16\n",
            "        Eval losses 3300 = NLL 3296 + KL IC 4,4 + KL II 0,0 + L2 0.16\n",
            "Batches 10801-10900 in 7.29 sec, Step size: 0.00113\n",
            "    Training losses 3385 = NLL 3381 + KL IC 4,4 + KL II 0,0 + L2 0.16\n",
            "        Eval losses 3325 = NLL 3321 + KL IC 4,4 + KL II 0,0 + L2 0.16\n",
            "Batches 10901-11000 in 7.19 sec, Step size: 0.00111\n",
            "    Training losses 3362 = NLL 3358 + KL IC 4,4 + KL II 0,0 + L2 0.16\n",
            "        Eval losses 3325 = NLL 3321 + KL IC 4,4 + KL II 0,0 + L2 0.16\n",
            "Batches 11001-11100 in 7.17 sec, Step size: 0.00109\n",
            "    Training losses 3390 = NLL 3386 + KL IC 4,4 + KL II 0,0 + L2 0.16\n",
            "        Eval losses 3286 = NLL 3283 + KL IC 4,4 + KL II 0,0 + L2 0.16\n",
            "Batches 11101-11200 in 7.16 sec, Step size: 0.00106\n",
            "    Training losses 3349 = NLL 3345 + KL IC 4,4 + KL II 0,0 + L2 0.16\n",
            "        Eval losses 3341 = NLL 3338 + KL IC 4,4 + KL II 0,0 + L2 0.16\n",
            "Batches 11201-11300 in 7.15 sec, Step size: 0.00104\n",
            "    Training losses 3383 = NLL 3379 + KL IC 4,4 + KL II 0,0 + L2 0.16\n",
            "        Eval losses 3302 = NLL 3298 + KL IC 4,4 + KL II 0,0 + L2 0.16\n",
            "Batches 11301-11400 in 7.16 sec, Step size: 0.00102\n",
            "    Training losses 3374 = NLL 3370 + KL IC 4,4 + KL II 0,0 + L2 0.16\n",
            "        Eval losses 3324 = NLL 3320 + KL IC 4,4 + KL II 0,0 + L2 0.16\n",
            "Batches 11401-11500 in 7.14 sec, Step size: 0.00100\n",
            "    Training losses 3344 = NLL 3339 + KL IC 4,4 + KL II 0,0 + L2 0.16\n",
            "        Eval losses 3327 = NLL 3323 + KL IC 4,4 + KL II 0,0 + L2 0.16\n",
            "Batches 11501-11600 in 7.13 sec, Step size: 0.00098\n",
            "    Training losses 3323 = NLL 3319 + KL IC 4,4 + KL II 0,0 + L2 0.16\n",
            "        Eval losses 3298 = NLL 3294 + KL IC 4,4 + KL II 0,0 + L2 0.16\n",
            "Batches 11601-11700 in 7.13 sec, Step size: 0.00096\n",
            "    Training losses 3380 = NLL 3375 + KL IC 4,4 + KL II 0,0 + L2 0.16\n",
            "        Eval losses 3305 = NLL 3301 + KL IC 4,4 + KL II 0,0 + L2 0.16\n",
            "Batches 11701-11800 in 7.13 sec, Step size: 0.00094\n",
            "    Training losses 3348 = NLL 3343 + KL IC 4,4 + KL II 0,0 + L2 0.16\n",
            "        Eval losses 3296 = NLL 3292 + KL IC 4,4 + KL II 0,0 + L2 0.16\n",
            "Batches 11801-11900 in 7.13 sec, Step size: 0.00093\n",
            "    Training losses 3342 = NLL 3338 + KL IC 4,4 + KL II 0,0 + L2 0.16\n",
            "        Eval losses 3303 = NLL 3298 + KL IC 4,4 + KL II 0,0 + L2 0.16\n",
            "Batches 11901-12000 in 7.13 sec, Step size: 0.00091\n",
            "    Training losses 3356 = NLL 3352 + KL IC 4,4 + KL II 0,0 + L2 0.16\n",
            "        Eval losses 3341 = NLL 3337 + KL IC 4,4 + KL II 0,0 + L2 0.16\n",
            "Batches 12001-12100 in 7.14 sec, Step size: 0.00089\n",
            "    Training losses 3390 = NLL 3386 + KL IC 4,4 + KL II 0,0 + L2 0.16\n",
            "        Eval losses 3299 = NLL 3294 + KL IC 4,4 + KL II 0,0 + L2 0.16\n",
            "Batches 12101-12200 in 7.14 sec, Step size: 0.00087\n",
            "    Training losses 3341 = NLL 3336 + KL IC 4,4 + KL II 0,0 + L2 0.16\n",
            "        Eval losses 3302 = NLL 3298 + KL IC 4,4 + KL II 0,0 + L2 0.16\n",
            "Batches 12201-12300 in 7.14 sec, Step size: 0.00085\n",
            "    Training losses 3371 = NLL 3367 + KL IC 4,4 + KL II 0,0 + L2 0.16\n",
            "        Eval losses 3309 = NLL 3305 + KL IC 4,4 + KL II 0,0 + L2 0.16\n",
            "Batches 12301-12400 in 7.14 sec, Step size: 0.00084\n",
            "    Training losses 3348 = NLL 3344 + KL IC 4,4 + KL II 0,0 + L2 0.16\n",
            "        Eval losses 3350 = NLL 3346 + KL IC 4,4 + KL II 0,0 + L2 0.16\n",
            "Batches 12401-12500 in 7.13 sec, Step size: 0.00082\n",
            "    Training losses 3361 = NLL 3357 + KL IC 4,4 + KL II 0,0 + L2 0.16\n",
            "        Eval losses 3268 = NLL 3264 + KL IC 4,4 + KL II 0,0 + L2 0.16\n",
            "Batches 12501-12600 in 7.14 sec, Step size: 0.00080\n",
            "    Training losses 3353 = NLL 3349 + KL IC 4,4 + KL II 0,0 + L2 0.16\n",
            "        Eval losses 3319 = NLL 3315 + KL IC 4,4 + KL II 0,0 + L2 0.16\n",
            "Batches 12601-12700 in 7.14 sec, Step size: 0.00079\n",
            "    Training losses 3340 = NLL 3336 + KL IC 4,4 + KL II 0,0 + L2 0.16\n",
            "        Eval losses 3355 = NLL 3351 + KL IC 4,4 + KL II 0,0 + L2 0.16\n",
            "Batches 12701-12800 in 7.25 sec, Step size: 0.00077\n",
            "    Training losses 3374 = NLL 3370 + KL IC 4,4 + KL II 0,0 + L2 0.16\n",
            "        Eval losses 3310 = NLL 3306 + KL IC 4,4 + KL II 0,0 + L2 0.16\n",
            "Batches 12801-12900 in 7.13 sec, Step size: 0.00076\n",
            "    Training losses 3324 = NLL 3319 + KL IC 4,4 + KL II 0,0 + L2 0.16\n",
            "        Eval losses 3316 = NLL 3312 + KL IC 4,4 + KL II 0,0 + L2 0.16\n",
            "Batches 12901-13000 in 7.14 sec, Step size: 0.00074\n",
            "    Training losses 3336 = NLL 3332 + KL IC 4,4 + KL II 0,0 + L2 0.16\n",
            "        Eval losses 3289 = NLL 3285 + KL IC 4,4 + KL II 0,0 + L2 0.16\n",
            "Batches 13001-13100 in 7.13 sec, Step size: 0.00073\n",
            "    Training losses 3345 = NLL 3341 + KL IC 4,4 + KL II 0,0 + L2 0.16\n",
            "        Eval losses 3310 = NLL 3306 + KL IC 4,4 + KL II 0,0 + L2 0.16\n",
            "Batches 13101-13200 in 7.14 sec, Step size: 0.00071\n",
            "    Training losses 3375 = NLL 3371 + KL IC 4,4 + KL II 0,0 + L2 0.16\n",
            "        Eval losses 3293 = NLL 3289 + KL IC 4,4 + KL II 0,0 + L2 0.16\n",
            "Batches 13201-13300 in 7.14 sec, Step size: 0.00070\n",
            "    Training losses 3366 = NLL 3362 + KL IC 5,5 + KL II 0,0 + L2 0.16\n",
            "        Eval losses 3299 = NLL 3295 + KL IC 4,4 + KL II 0,0 + L2 0.16\n",
            "Batches 13301-13400 in 7.14 sec, Step size: 0.00069\n",
            "    Training losses 3360 = NLL 3355 + KL IC 5,5 + KL II 0,0 + L2 0.16\n",
            "        Eval losses 3309 = NLL 3305 + KL IC 4,4 + KL II 0,0 + L2 0.16\n",
            "Batches 13401-13500 in 7.14 sec, Step size: 0.00067\n",
            "    Training losses 3372 = NLL 3367 + KL IC 5,5 + KL II 0,0 + L2 0.16\n",
            "        Eval losses 3318 = NLL 3313 + KL IC 4,4 + KL II 0,0 + L2 0.16\n",
            "Batches 13501-13600 in 7.14 sec, Step size: 0.00066\n",
            "    Training losses 3341 = NLL 3337 + KL IC 4,4 + KL II 0,0 + L2 0.16\n",
            "        Eval losses 3319 = NLL 3315 + KL IC 4,4 + KL II 0,0 + L2 0.16\n",
            "Batches 13601-13700 in 7.14 sec, Step size: 0.00065\n",
            "    Training losses 3361 = NLL 3356 + KL IC 4,4 + KL II 0,0 + L2 0.16\n",
            "        Eval losses 3290 = NLL 3286 + KL IC 4,4 + KL II 0,0 + L2 0.16\n",
            "Batches 13701-13800 in 7.14 sec, Step size: 0.00063\n",
            "    Training losses 3340 = NLL 3336 + KL IC 5,5 + KL II 0,0 + L2 0.16\n",
            "        Eval losses 3323 = NLL 3318 + KL IC 4,4 + KL II 0,0 + L2 0.16\n",
            "Batches 13801-13900 in 7.14 sec, Step size: 0.00062\n",
            "    Training losses 3351 = NLL 3346 + KL IC 4,4 + KL II 0,0 + L2 0.16\n",
            "        Eval losses 3331 = NLL 3326 + KL IC 4,4 + KL II 0,0 + L2 0.16\n",
            "Batches 13901-14000 in 7.19 sec, Step size: 0.00061\n",
            "    Training losses 3337 = NLL 3332 + KL IC 5,5 + KL II 0,0 + L2 0.16\n",
            "        Eval losses 3305 = NLL 3300 + KL IC 4,4 + KL II 0,0 + L2 0.16\n",
            "Batches 14001-14100 in 7.26 sec, Step size: 0.00060\n",
            "    Training losses 3365 = NLL 3361 + KL IC 4,4 + KL II 0,0 + L2 0.16\n",
            "        Eval losses 3305 = NLL 3301 + KL IC 4,4 + KL II 0,0 + L2 0.16\n",
            "Batches 14101-14200 in 7.28 sec, Step size: 0.00058\n",
            "    Training losses 3347 = NLL 3343 + KL IC 4,4 + KL II 0,0 + L2 0.17\n",
            "        Eval losses 3326 = NLL 3322 + KL IC 4,4 + KL II 0,0 + L2 0.17\n",
            "Batches 14201-14300 in 7.27 sec, Step size: 0.00057\n",
            "    Training losses 3346 = NLL 3341 + KL IC 4,4 + KL II 0,0 + L2 0.17\n",
            "        Eval losses 3309 = NLL 3305 + KL IC 4,4 + KL II 0,0 + L2 0.17\n",
            "Batches 14301-14400 in 7.14 sec, Step size: 0.00056\n",
            "    Training losses 3361 = NLL 3357 + KL IC 4,4 + KL II 0,0 + L2 0.17\n",
            "        Eval losses 3342 = NLL 3337 + KL IC 4,4 + KL II 0,0 + L2 0.17\n",
            "Batches 14401-14500 in 7.30 sec, Step size: 0.00055\n",
            "    Training losses 3337 = NLL 3332 + KL IC 4,4 + KL II 0,0 + L2 0.17\n",
            "        Eval losses 3290 = NLL 3286 + KL IC 4,4 + KL II 0,0 + L2 0.17\n",
            "Batches 14501-14600 in 7.61 sec, Step size: 0.00054\n",
            "    Training losses 3337 = NLL 3333 + KL IC 4,4 + KL II 0,0 + L2 0.17\n",
            "        Eval losses 3290 = NLL 3286 + KL IC 4,4 + KL II 0,0 + L2 0.17\n",
            "Batches 14601-14700 in 7.28 sec, Step size: 0.00053\n",
            "    Training losses 3377 = NLL 3372 + KL IC 4,4 + KL II 0,0 + L2 0.17\n",
            "        Eval losses 3336 = NLL 3332 + KL IC 4,4 + KL II 0,0 + L2 0.17\n",
            "Batches 14701-14800 in 7.15 sec, Step size: 0.00052\n",
            "    Training losses 3347 = NLL 3342 + KL IC 5,5 + KL II 0,0 + L2 0.17\n",
            "        Eval losses 3302 = NLL 3297 + KL IC 4,4 + KL II 0,0 + L2 0.17\n",
            "Batches 14801-14900 in 7.15 sec, Step size: 0.00051\n",
            "    Training losses 3370 = NLL 3365 + KL IC 5,5 + KL II 0,0 + L2 0.17\n",
            "        Eval losses 3310 = NLL 3306 + KL IC 4,4 + KL II 0,0 + L2 0.17\n",
            "Batches 14901-15000 in 7.15 sec, Step size: 0.00050\n",
            "    Training losses 3372 = NLL 3368 + KL IC 4,4 + KL II 0,0 + L2 0.17\n",
            "        Eval losses 3297 = NLL 3293 + KL IC 4,4 + KL II 0,0 + L2 0.17\n",
            "Batches 15001-15100 in 7.16 sec, Step size: 0.00049\n",
            "    Training losses 3325 = NLL 3321 + KL IC 4,4 + KL II 0,0 + L2 0.17\n",
            "        Eval losses 3323 = NLL 3319 + KL IC 4,4 + KL II 0,0 + L2 0.17\n",
            "Batches 15101-15200 in 7.17 sec, Step size: 0.00048\n",
            "    Training losses 3346 = NLL 3341 + KL IC 4,4 + KL II 0,0 + L2 0.17\n",
            "        Eval losses 3317 = NLL 3312 + KL IC 4,4 + KL II 0,0 + L2 0.17\n",
            "Batches 15201-15300 in 7.16 sec, Step size: 0.00047\n",
            "    Training losses 3351 = NLL 3346 + KL IC 4,4 + KL II 0,0 + L2 0.17\n",
            "        Eval losses 3283 = NLL 3279 + KL IC 4,4 + KL II 0,0 + L2 0.17\n",
            "Batches 15301-15400 in 7.15 sec, Step size: 0.00046\n",
            "    Training losses 3380 = NLL 3375 + KL IC 4,4 + KL II 0,0 + L2 0.17\n",
            "        Eval losses 3298 = NLL 3294 + KL IC 4,4 + KL II 0,0 + L2 0.17\n",
            "Batches 15401-15500 in 7.14 sec, Step size: 0.00045\n",
            "    Training losses 3388 = NLL 3384 + KL IC 4,4 + KL II 0,0 + L2 0.17\n",
            "        Eval losses 3317 = NLL 3313 + KL IC 4,4 + KL II 0,0 + L2 0.17\n",
            "Batches 15501-15600 in 7.16 sec, Step size: 0.00044\n",
            "    Training losses 3350 = NLL 3346 + KL IC 4,4 + KL II 0,0 + L2 0.17\n",
            "        Eval losses 3309 = NLL 3305 + KL IC 4,4 + KL II 0,0 + L2 0.17\n",
            "Batches 15601-15700 in 7.15 sec, Step size: 0.00043\n",
            "    Training losses 3351 = NLL 3346 + KL IC 4,4 + KL II 0,0 + L2 0.17\n",
            "        Eval losses 3301 = NLL 3297 + KL IC 4,4 + KL II 0,0 + L2 0.17\n",
            "Batches 15701-15800 in 7.16 sec, Step size: 0.00042\n",
            "    Training losses 3311 = NLL 3306 + KL IC 4,4 + KL II 0,0 + L2 0.17\n",
            "        Eval losses 3275 = NLL 3271 + KL IC 4,4 + KL II 0,0 + L2 0.17\n",
            "Batches 15801-15900 in 7.16 sec, Step size: 0.00042\n",
            "    Training losses 3342 = NLL 3338 + KL IC 4,4 + KL II 0,0 + L2 0.17\n",
            "        Eval losses 3305 = NLL 3301 + KL IC 4,4 + KL II 0,0 + L2 0.17\n",
            "Batches 15901-16000 in 7.15 sec, Step size: 0.00041\n",
            "    Training losses 3390 = NLL 3385 + KL IC 4,4 + KL II 0,0 + L2 0.17\n",
            "        Eval losses 3312 = NLL 3308 + KL IC 4,4 + KL II 0,0 + L2 0.17\n",
            "Batches 16001-16100 in 7.15 sec, Step size: 0.00040\n",
            "    Training losses 3329 = NLL 3325 + KL IC 4,4 + KL II 0,0 + L2 0.17\n",
            "        Eval losses 3295 = NLL 3290 + KL IC 4,4 + KL II 0,0 + L2 0.17\n",
            "Batches 16101-16200 in 7.15 sec, Step size: 0.00039\n",
            "    Training losses 3367 = NLL 3362 + KL IC 5,5 + KL II 0,0 + L2 0.17\n",
            "        Eval losses 3274 = NLL 3269 + KL IC 4,4 + KL II 0,0 + L2 0.17\n",
            "Batches 16201-16300 in 7.16 sec, Step size: 0.00038\n",
            "    Training losses 3356 = NLL 3351 + KL IC 5,5 + KL II 0,0 + L2 0.17\n",
            "        Eval losses 3279 = NLL 3274 + KL IC 4,4 + KL II 0,0 + L2 0.17\n",
            "Batches 16301-16400 in 7.28 sec, Step size: 0.00038\n",
            "    Training losses 3363 = NLL 3358 + KL IC 4,4 + KL II 0,0 + L2 0.17\n",
            "        Eval losses 3293 = NLL 3289 + KL IC 4,4 + KL II 0,0 + L2 0.17\n",
            "Batches 16401-16500 in 7.16 sec, Step size: 0.00037\n",
            "    Training losses 3358 = NLL 3353 + KL IC 5,5 + KL II 0,0 + L2 0.17\n",
            "        Eval losses 3280 = NLL 3275 + KL IC 4,4 + KL II 0,0 + L2 0.17\n",
            "Batches 16501-16600 in 7.16 sec, Step size: 0.00036\n",
            "    Training losses 3375 = NLL 3370 + KL IC 4,4 + KL II 0,0 + L2 0.17\n",
            "        Eval losses 3332 = NLL 3328 + KL IC 4,4 + KL II 0,0 + L2 0.17\n",
            "Batches 16601-16700 in 7.17 sec, Step size: 0.00035\n",
            "    Training losses 3326 = NLL 3322 + KL IC 4,4 + KL II 0,0 + L2 0.17\n",
            "        Eval losses 3317 = NLL 3312 + KL IC 4,4 + KL II 0,0 + L2 0.17\n",
            "Batches 16701-16800 in 7.16 sec, Step size: 0.00035\n",
            "    Training losses 3347 = NLL 3343 + KL IC 4,4 + KL II 0,0 + L2 0.17\n",
            "        Eval losses 3271 = NLL 3267 + KL IC 4,4 + KL II 0,0 + L2 0.17\n",
            "Batches 16801-16900 in 7.14 sec, Step size: 0.00034\n",
            "    Training losses 3334 = NLL 3329 + KL IC 4,4 + KL II 0,0 + L2 0.17\n",
            "        Eval losses 3300 = NLL 3296 + KL IC 4,4 + KL II 0,0 + L2 0.17\n",
            "Batches 16901-17000 in 7.13 sec, Step size: 0.00033\n",
            "    Training losses 3325 = NLL 3320 + KL IC 4,4 + KL II 0,0 + L2 0.17\n",
            "        Eval losses 3305 = NLL 3300 + KL IC 4,4 + KL II 0,0 + L2 0.17\n",
            "Batches 17001-17100 in 7.13 sec, Step size: 0.00033\n",
            "    Training losses 3340 = NLL 3336 + KL IC 5,5 + KL II 0,0 + L2 0.17\n",
            "        Eval losses 3332 = NLL 3327 + KL IC 4,4 + KL II 0,0 + L2 0.17\n",
            "Batches 17101-17200 in 7.13 sec, Step size: 0.00032\n",
            "    Training losses 3346 = NLL 3342 + KL IC 4,4 + KL II 0,0 + L2 0.17\n",
            "        Eval losses 3313 = NLL 3308 + KL IC 4,4 + KL II 0,0 + L2 0.17\n",
            "Batches 17201-17300 in 7.13 sec, Step size: 0.00031\n",
            "    Training losses 3390 = NLL 3385 + KL IC 4,4 + KL II 0,0 + L2 0.17\n",
            "        Eval losses 3307 = NLL 3302 + KL IC 4,4 + KL II 0,0 + L2 0.17\n",
            "Batches 17301-17400 in 7.13 sec, Step size: 0.00031\n",
            "    Training losses 3349 = NLL 3345 + KL IC 4,4 + KL II 0,0 + L2 0.17\n",
            "        Eval losses 3304 = NLL 3300 + KL IC 4,4 + KL II 0,0 + L2 0.17\n",
            "Batches 17401-17500 in 7.14 sec, Step size: 0.00030\n",
            "    Training losses 3381 = NLL 3376 + KL IC 4,4 + KL II 0,0 + L2 0.17\n",
            "        Eval losses 3333 = NLL 3329 + KL IC 4,4 + KL II 0,0 + L2 0.17\n",
            "Batches 17501-17600 in 7.14 sec, Step size: 0.00030\n",
            "    Training losses 3387 = NLL 3382 + KL IC 5,5 + KL II 0,0 + L2 0.17\n",
            "        Eval losses 3314 = NLL 3309 + KL IC 4,4 + KL II 0,0 + L2 0.17\n",
            "Batches 17601-17700 in 7.14 sec, Step size: 0.00029\n",
            "    Training losses 3352 = NLL 3347 + KL IC 5,5 + KL II 0,0 + L2 0.17\n",
            "        Eval losses 3286 = NLL 3282 + KL IC 4,4 + KL II 0,0 + L2 0.17\n",
            "Batches 17701-17800 in 7.14 sec, Step size: 0.00028\n",
            "    Training losses 3376 = NLL 3372 + KL IC 5,5 + KL II 0,0 + L2 0.17\n",
            "        Eval losses 3295 = NLL 3291 + KL IC 4,4 + KL II 0,0 + L2 0.17\n",
            "Batches 17801-17900 in 7.14 sec, Step size: 0.00028\n",
            "    Training losses 3360 = NLL 3355 + KL IC 5,5 + KL II 0,0 + L2 0.17\n",
            "        Eval losses 3305 = NLL 3300 + KL IC 4,4 + KL II 0,0 + L2 0.17\n",
            "Batches 17901-18000 in 7.15 sec, Step size: 0.00027\n",
            "    Training losses 3360 = NLL 3355 + KL IC 5,5 + KL II 0,0 + L2 0.17\n",
            "        Eval losses 3284 = NLL 3279 + KL IC 5,5 + KL II 0,0 + L2 0.17\n",
            "Batches 18001-18100 in 7.14 sec, Step size: 0.00027\n",
            "    Training losses 3355 = NLL 3350 + KL IC 5,5 + KL II 0,0 + L2 0.17\n",
            "        Eval losses 3283 = NLL 3279 + KL IC 5,5 + KL II 0,0 + L2 0.17\n",
            "Batches 18101-18200 in 7.17 sec, Step size: 0.00026\n",
            "    Training losses 3324 = NLL 3319 + KL IC 5,5 + KL II 0,0 + L2 0.17\n",
            "        Eval losses 3329 = NLL 3324 + KL IC 5,5 + KL II 0,0 + L2 0.17\n",
            "Batches 18201-18300 in 7.20 sec, Step size: 0.00026\n",
            "    Training losses 3395 = NLL 3391 + KL IC 5,5 + KL II 0,0 + L2 0.17\n",
            "        Eval losses 3292 = NLL 3287 + KL IC 5,5 + KL II 0,0 + L2 0.17\n",
            "Batches 18301-18400 in 7.14 sec, Step size: 0.00025\n",
            "    Training losses 3371 = NLL 3366 + KL IC 5,5 + KL II 0,0 + L2 0.17\n",
            "        Eval losses 3288 = NLL 3283 + KL IC 4,4 + KL II 0,0 + L2 0.17\n",
            "Batches 18401-18500 in 7.15 sec, Step size: 0.00025\n",
            "    Training losses 3326 = NLL 3321 + KL IC 5,5 + KL II 0,0 + L2 0.17\n",
            "        Eval losses 3304 = NLL 3300 + KL IC 5,5 + KL II 0,0 + L2 0.17\n",
            "Batches 18501-18600 in 7.16 sec, Step size: 0.00024\n",
            "    Training losses 3334 = NLL 3329 + KL IC 5,5 + KL II 0,0 + L2 0.17\n",
            "        Eval losses 3291 = NLL 3287 + KL IC 5,5 + KL II 0,0 + L2 0.17\n",
            "Batches 18601-18700 in 7.15 sec, Step size: 0.00024\n",
            "    Training losses 3340 = NLL 3335 + KL IC 5,5 + KL II 0,0 + L2 0.17\n",
            "        Eval losses 3285 = NLL 3280 + KL IC 5,5 + KL II 0,0 + L2 0.17\n",
            "Batches 18701-18800 in 7.17 sec, Step size: 0.00023\n",
            "    Training losses 3368 = NLL 3363 + KL IC 5,5 + KL II 0,0 + L2 0.17\n",
            "        Eval losses 3302 = NLL 3297 + KL IC 5,5 + KL II 0,0 + L2 0.17\n",
            "Batches 18801-18900 in 7.14 sec, Step size: 0.00023\n",
            "    Training losses 3330 = NLL 3325 + KL IC 5,5 + KL II 0,0 + L2 0.17\n",
            "        Eval losses 3301 = NLL 3296 + KL IC 5,5 + KL II 0,0 + L2 0.17\n",
            "Batches 18901-19000 in 7.13 sec, Step size: 0.00022\n",
            "    Training losses 3327 = NLL 3322 + KL IC 5,5 + KL II 0,0 + L2 0.17\n",
            "        Eval losses 3297 = NLL 3292 + KL IC 5,5 + KL II 0,0 + L2 0.17\n",
            "Batches 19001-19100 in 7.13 sec, Step size: 0.00022\n",
            "    Training losses 3331 = NLL 3326 + KL IC 5,5 + KL II 0,0 + L2 0.17\n",
            "        Eval losses 3310 = NLL 3305 + KL IC 5,5 + KL II 0,0 + L2 0.17\n",
            "Batches 19101-19200 in 7.15 sec, Step size: 0.00021\n",
            "    Training losses 3322 = NLL 3316 + KL IC 5,5 + KL II 0,0 + L2 0.17\n",
            "        Eval losses 3312 = NLL 3308 + KL IC 5,5 + KL II 0,0 + L2 0.17\n",
            "Batches 19201-19300 in 7.14 sec, Step size: 0.00021\n",
            "    Training losses 3368 = NLL 3363 + KL IC 5,5 + KL II 0,0 + L2 0.17\n",
            "        Eval losses 3313 = NLL 3308 + KL IC 5,5 + KL II 0,0 + L2 0.17\n",
            "Batches 19301-19400 in 7.13 sec, Step size: 0.00021\n",
            "    Training losses 3338 = NLL 3332 + KL IC 5,5 + KL II 0,0 + L2 0.17\n",
            "        Eval losses 3303 = NLL 3298 + KL IC 5,5 + KL II 0,0 + L2 0.17\n",
            "Batches 19401-19500 in 7.13 sec, Step size: 0.00020\n",
            "    Training losses 3323 = NLL 3319 + KL IC 5,5 + KL II 0,0 + L2 0.17\n",
            "        Eval losses 3310 = NLL 3305 + KL IC 5,5 + KL II 0,0 + L2 0.17\n",
            "Batches 19501-19600 in 7.14 sec, Step size: 0.00020\n",
            "    Training losses 3369 = NLL 3364 + KL IC 5,5 + KL II 0,0 + L2 0.17\n",
            "        Eval losses 3310 = NLL 3305 + KL IC 5,5 + KL II 0,0 + L2 0.17\n",
            "Batches 19601-19700 in 7.13 sec, Step size: 0.00019\n",
            "    Training losses 3314 = NLL 3309 + KL IC 5,5 + KL II 0,0 + L2 0.17\n",
            "        Eval losses 3318 = NLL 3314 + KL IC 5,5 + KL II 0,0 + L2 0.17\n",
            "Batches 19701-19800 in 7.15 sec, Step size: 0.00019\n",
            "    Training losses 3351 = NLL 3346 + KL IC 5,5 + KL II 0,0 + L2 0.17\n",
            "        Eval losses 3291 = NLL 3286 + KL IC 5,5 + KL II 0,0 + L2 0.17\n",
            "Batches 19801-19900 in 7.14 sec, Step size: 0.00019\n",
            "    Training losses 3349 = NLL 3344 + KL IC 5,5 + KL II 0,0 + L2 0.17\n",
            "        Eval losses 3321 = NLL 3316 + KL IC 4,4 + KL II 0,0 + L2 0.17\n",
            "Batches 19901-20000 in 7.16 sec, Step size: 0.00018\n",
            "    Training losses 3349 = NLL 3344 + KL IC 5,5 + KL II 0,0 + L2 0.17\n",
            "        Eval losses 3311 = NLL 3306 + KL IC 4,4 + KL II 0,0 + L2 0.17\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fname_uniquifier = datetime.datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
        "network_fname = ('trained_params_lfads_129_gru' + fname_uniquifier + '.npz')\n",
        "network_path = os.path.join(output_dir, network_fname)\n",
        "\n",
        "print(\"Saving parameters: \", network_path)\n",
        "onp.savez(network_path, trained_params)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rhrKd0T_pFEC",
        "outputId": "727fd24d-1866-4055-ff74-430bcfe060e1"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving parameters:  /content/lfads/output/trained_params_lfads_129_gru2022-08-02_16:53:22.npz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the training details\n",
        "x = onp.arange(0, num_batches, print_every)\n",
        "plt.figure(figsize=(20,6))\n",
        "plt.subplot(251)\n",
        "plt.plot(x, opt_details['tlosses']['total'], 'k')\n",
        "plt.ylabel('Training')\n",
        "plt.title('Total loss')\n",
        "plt.subplot(252)\n",
        "plt.plot(x, opt_details['tlosses']['nlog_p_xgz'], 'b')\n",
        "plt.title('Negative log p(z|x)')\n",
        "plt.subplot(253)\n",
        "plt.plot(x, opt_details['tlosses']['kl_ii'], 'r')\n",
        "plt.title('KL inferred inputs')\n",
        "plt.subplot(254)\n",
        "plt.plot(x, opt_details['tlosses']['kl_g0'], 'g')\n",
        "plt.title('KL initial state')\n",
        "plt.subplot(255)\n",
        "plt.plot(x, opt_details['tlosses']['l2'], 'c')\n",
        "plt.xlabel('Training batch')\n",
        "plt.title('L2 loss')\n",
        "plt.subplot(256)\n",
        "plt.plot(x, opt_details['elosses']['total'], 'k')\n",
        "plt.xlabel('Training batch')\n",
        "plt.ylabel('Evaluation')\n",
        "plt.subplot(257)\n",
        "plt.plot(x, opt_details['tlosses']['nlog_p_xgz'], 'b')\n",
        "plt.xlabel('Training batch')\n",
        "plt.subplot(258)\n",
        "plt.plot(x, opt_details['elosses']['kl_ii'], 'r')\n",
        "plt.xlabel('Training batch')\n",
        "plt.subplot(259)\n",
        "plt.plot(x, opt_details['elosses']['kl_g0'], 'g')\n",
        "plt.xlabel('Training batch');"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "id": "JLo5QVPM76Eh",
        "outputId": "50f7a992-b0e2-4f34-f32e-3d387ca688cd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-06f5ad411d3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Plot the training details\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0monp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m251\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_details\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tlosses'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'total'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'k'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'onp' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# See the effect of the KL warmup, which is shown \n",
        "# by the KL penalities without the warmup scaling. \n",
        "plt.figure(figsize=(7,4))\n",
        "plt.subplot(221)\n",
        "plt.plot(x, opt_details['tlosses']['kl_ii_prescale'], 'r--')\n",
        "plt.ylabel('Training')\n",
        "plt.subplot(222)\n",
        "plt.plot(x, opt_details['tlosses']['kl_g0_prescale'], 'g--')\n",
        "plt.subplot(223)\n",
        "plt.plot(x, opt_details['elosses']['kl_ii_prescale'], 'r--')\n",
        "plt.ylabel('Evaluation')\n",
        "plt.xlabel('Training batch')\n",
        "plt.subplot(224)\n",
        "plt.plot(x, opt_details['elosses']['kl_g0_prescale'], 'g--')\n",
        "plt.xlabel('Training batch');"
      ],
      "metadata": {
        "id": "_g5rtV3EQBM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualization"
      ],
      "metadata": {
        "id": "RiEJTNiKQUOf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot a bunch of examples of eval trials run through LFADS.\n",
        "reload(plotting)\n",
        "#reload(lfads)\n",
        "\n",
        "def plot_rescale_fun(a): \n",
        "    fac = max_firing_rate * data_dt\n",
        "    return renormed_fun(a) * fac\n",
        "\n",
        "\n",
        "bidx = 0\n",
        "\n",
        "nexamples_to_save = 1\n",
        "for eidx in range(nexamples_to_save):\n",
        "    fkey = random.fold_in(key, eidx)\n",
        "    psa_example = eval_data[bidx,:,:].astype(np.float32)\n",
        "    psa_dict = lfads.posterior_sample_and_average_jit(trained_params, lfads_hps, \n",
        "                                                      fkey, psa_example, gen=model)\n",
        "\n",
        "    # The inferred input and true input are rescaled and shifted via \n",
        "    # linear regression to match, as there is an identifiability issue. there.\n",
        "    # plotting.plot_lfads(psa_example, psa_dict,\n",
        "    #                     data_dict, eval_data_offset+bidx, plot_rescale_fun)\n"
      ],
      "metadata": {
        "id": "lX7eD0ozQLmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plt.plot(psa_dict['factor_t'])\n",
        "plt.plot(psa_dict['factor_t'])\n",
        "psa_dict.keys()"
      ],
      "metadata": {
        "id": "JXh3d4vBQWBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ics = onp.empty((data_bxtxn.shape[0],gen_dim))\n",
        "gen_traj = onp.empty((data_bxtxn.shape[0],ntimesteps,gen_dim))\n",
        "for i, psa_example in enumerate(data_bxtxn.astype(np.float32)):\n",
        "    psa_dict = lfads.posterior_sample_and_average_jit(trained_params, lfads_hps, \n",
        "                                                      fkey, psa_example, gen=model)\n",
        "    ics[i] = psa_dict['ic_mean']\n",
        "    gen_traj[i] = psa_dict['gen_t']"
      ],
      "metadata": {
        "id": "tPgWndF3Qd7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conds = dataset.trial_info.set_index(['trial_type', 'trial_version']).index.unique().tolist()\n",
        "\n",
        "# Loop over conditions and compute average trajectory\n",
        "reach_angle = onp.empty(len(dataset.trial_info)).astype('float32')\n",
        "for cond in conds:\n",
        "    # Find trials in condition\n",
        "    mask = onp.all(dataset.trial_info[['trial_type', 'trial_version']] == cond, axis=1)\n",
        "    # Determine reach angle for color\n",
        "    active_target = dataset.trial_info[mask].target_pos.iloc[0][dataset.trial_info[mask].active_target.iloc[0]]\n",
        "    reach_angle[mask] = onp.arctan2(*active_target[::-1])\n",
        "\n",
        "\n",
        "ncomponents = 3\n",
        "pca = sklearn.decomposition.PCA(ncomponents)\n",
        "pca.fit(onp.reshape(gen_traj, [-1, gen_dim]))\n",
        "\n",
        "S = pca.components_.T\n",
        "\n",
        "for a,t in zip(reach_angle,gen_traj):\n",
        "  plt.plot(*(t@S)[:,[1,2]].T, c=plt.cm.rainbow(a / (2*onp.pi) + 0.5),alpha=0.5)"
      ],
      "metadata": {
        "id": "QEK3_yJf3LKw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fig = plt.figure()\n",
        "# ax = plt.axes(projection='3d')\n",
        "# for a,t in zip(reach_angle,gen_traj):\n",
        "#   plt.plot(*(t@S).T, c=plt.cm.rainbow(a / (2*onp.pi) + 0.5),alpha=0.5)\n",
        "# ax.view_init(-120, 120)\n",
        "\n",
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "\n",
        "# print((gen_traj@S)[:,0])\n",
        "\n",
        "df = pd.DataFrame.from_dict({'PC0': (gen_traj@S)[...,0].flatten(),\n",
        "                             'PC1': (gen_traj@S)[...,1].flatten(),\n",
        "                             'PC2': (gen_traj@S)[...,2].flatten(),\n",
        "                             'color': reach_angle.repeat(ntimesteps).flatten()})\n",
        "\n",
        "fig = px.scatter_3d(df, x=\"PC0\", y=\"PC1\", z=\"PC2\",color='color')\n",
        "fig.update_traces(marker=dict(size=3,\n",
        "                              line=dict(width=.01,\n",
        "                                        color='DarkSlateGrey')),\n",
        "                  selector=dict(mode='markers'))\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "VVYuSVN95Fmx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "ncomponents = 3\n",
        "tsne = TSNE(ncomponents,perplexity=40)\n",
        "embedded_ics = tsne.fit_transform(ics)\n",
        "\n",
        "plt.scatter(*embedded_ics[:,:2].T, c=plt.cm.rainbow(reach_angle[:] / (2*onp.pi) + 0.5),alpha=0.5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "id": "7BMhOvteRINg",
        "outputId": "4f9d65f1-7314-42c2-d8e7-a1032206f83f"
      },
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/manifold/_t_sne.py:783: FutureWarning:\n",
            "\n",
            "The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/manifold/_t_sne.py:793: FutureWarning:\n",
            "\n",
            "The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7f932a76b110>"
            ]
          },
          "metadata": {},
          "execution_count": 168
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(trained_params.keys())\n",
        "trained_params['gen'].keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uahsJ9YDRDq3",
        "outputId": "3ba3b7a2-c19d-43be-da83-c2363cf784c3"
      },
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['con', 'con_out', 'factors', 'gen', 'gen_ic', 'ic_enc', 'ic_prior', 'ii_prior', 'logrates'])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['bC', 'bRU', 'h0', 'wCHX', 'wRUHX'])"
            ]
          },
          "metadata": {},
          "execution_count": 169
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(4,4))\n",
        "eigv = onp.linalg.eig(trained_params['gen']['wA'])[0]\n",
        "plt.scatter(eigv.real,eigv.imag)\n",
        "x = np.linspace(-np.pi,np.pi,100)\n",
        "plt.plot(np.sin(x),np.cos(x),c='k')\n",
        "# plt.plot(x,np.cos(x))\n",
        "scale = onp.sqrt(eigv.real**2 + eigv.imag**2).max()*1.05\n",
        "plt.xlim([-scale,+scale])\n",
        "plt.ylim([-scale,+scale])"
      ],
      "metadata": {
        "id": "Clhi6xH2SR4K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "outputId": "21264601-2f5a-42fc-f32b-0bf03be4ac22"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-141-7800c194995a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0meigv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0monp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrained_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'gen'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'wA'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meigv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreal\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0meigv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'k'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'wA'"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 288x288 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.min(trained_params['gen']['wA'])"
      ],
      "metadata": {
        "id": "00LGDMKdrdQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "kfAu8zQjM5kY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References\n",
        "\n",
        "1. *Our review on fully observed vs latent dynamical models*: Hurwitz, Cole, et al. [Building population models for large-scale neural recordings: Opportunities and pitfalls.](https://arxiv.org/pdf/2102.01807.pdf) Current Opinion in Neurobiology 70 (2021): 64-73.\n",
        "  \n",
        "## Interesting recent papers for further reading\n",
        "\n",
        "If you want to know what's going on in the field right now, you can check the following papers (a very biased selection from Nina ;) ):\n",
        "\n",
        "1. Smith, Jimmy, Scott Linderman, and David Sussillo. [Reverse engineering recurrent neural networks with jacobian switching linear dynamical systems.](https://arxiv.org/pdf/2111.01256.pdf) Advances in Neural Information Processing Systems 34 (2021): 16700-16713.\n",
        "2. "
      ],
      "metadata": {
        "id": "TFRVmFtnnm36"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "xhBbaNsprzlS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}