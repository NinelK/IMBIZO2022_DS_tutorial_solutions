{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DS tutorial",
      "provenance": [],
      "collapsed_sections": [
        "5lSgytL2f3OV",
        "svmBduOG_52v",
        "qIm6cjLqfaWB",
        "TnpeAv5pjQr4"
      ],
      "authorship_tag": "ABX9TyOi5rrtAToyJ2jULA7gFFYS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NinelK/SA_DS_tutorial/blob/main/DS_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tutorial:** Dynamical systems in neuroscience"
      ],
      "metadata": {
        "id": "9S_38ayWhm5W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "Dynamical systems are systems that **evolve** in time. These can be physical systems, economic systems, neurons, neural networks or the whole brain. No matter what the system is, the universal dynamical system framework can help us understand its time-dependent behavior.\n",
        "\n",
        "When modelling dynamics, we can represent time in two different ways: *discrete* or *continuous*. In both cases, we can write down the evolution of the system as a function of its previous `state' $x$ and external inputs that the system receives $u$:\n",
        "\n",
        "Discrete time systems               |           Continuous time systems\n",
        "------------------------------------|-------------------------------------\n",
        " $$x_{t+1} = F_d(x_t,u_t)$$           |    $$\\frac{dx(t)}{dt} = F_c(x(t),u(t))$$\n",
        "\n",
        "Here, the state $x$ can, for instance, correspont to neural firing. In discrete case, $x_t$ would be the number of spikes emmited within one time bin, while $x(t)$ could be a function which is only non-zero at spike times.\n",
        "External inputs $u$ in this case would correspond to the inputs from other neurons.\n",
        "\n",
        "The key part of the dynamical systems framework is the **evolution operator** $F$. Whether continuous $F_c$ or discrete $F_d$, it can tell us a lot about the system:\n",
        "> The power of the dynamical systems approach to neuroscience, as well as to many other sciences, is that we can tell something, or many things, about a system without knowing all the details that govern the system evolution. We do not even use equations to do that! Some may even wonder why we call it a mathematical theory.        *Eugene Izhikevich [1]*\n",
        "\n",
        "Therefore, the main goal of the tutorial is to learn how to tell something about the dynamical system knowing $F$."
      ],
      "metadata": {
        "id": "5lSgytL2f3OV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1: Autonomous linear dynamical systems"
      ],
      "metadata": {
        "id": "N_XDFK9Xe9Sj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sometimes, we can assume that the system does not receive any external inputs and evolves in time on its own ( $u(t)=0$ ). Such dynamical systems are called **autonomous**. Such systems are common in physics (e.g. swinging pendulum), but also surprisingly applicable to some biological neural networks, as we will see in the last part of the tutorial. Lack of external inputs greatly simplifies the analysis of the evolution $F$, so let us assume no external inputs for now.\n",
        "\n",
        "Let us start "
      ],
      "metadata": {
        "id": "1-CiIYrZ9wCc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Exercise 1**: relationship between continuous time and discrete time linear dynamical systems\n",
        "\n",
        "Suppose we know the state $x(0)$ of a continuous-time dynamical system and we want to know what happens next. To figure this out, we need to integrate a continuous-time dynamical system equation over time."
      ],
      "metadata": {
        "id": "svmBduOG_52v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1 objectives\n",
        "\n",
        "1. Learn what linear systems can and can not do\n",
        "2. Learn what fixed points are\n",
        "3. Read eignespectra"
      ],
      "metadata": {
        "id": "qIm6cjLqfaWB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2: Nonlinear dynamical systems"
      ],
      "metadata": {
        "id": "_8WO8jlefRx0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fitzhugh-Nagumo model of a spiking neuron\n",
        "\n",
        "Hodgkin Huxley -> simplify\n",
        "\n",
        "Nullclines\n",
        "\n",
        "Phase portrait"
      ],
      "metadata": {
        "id": "TnpeAv5pjQr4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3: Latent dynamics models"
      ],
      "metadata": {
        "id": "IcFzSH01fUIg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In recent years, progress in recording technologies enabled recordings of 100s to 1000s of neurons simultaneously.\n",
        "\n",
        "At the same time, it was shown that neural population activity often has a low-dimensional structure: a low number of latent dynamical factors can explain a large fraction of neural variability. This finding is called a 'manifold hypothesis', and was proposed in [REF].\n",
        "\n"
      ],
      "metadata": {
        "id": "6gsYQuo-gxU7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "WRITE INTRO\n",
        "\n",
        "Define Latent dynamics and Emission model"
      ],
      "metadata": {
        "id": "6XVO4mR3Hw5C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recurrent neural networks (RNNs)\n",
        "\n",
        "Let us first consider the simplest, yet most important example of a non-linear discrete-time latent dynamical system: recurrent neural network. It is a go-to tool for modelling sequences in deep learning.\n",
        "\n",
        "Latent dynamics: $$h_{t} = \\sigma (V h_{t-1} + U x_t + b_h)$$\n",
        "Emission model: $$o_t = W h_t + b_o $$\n",
        "\n",
        "\n",
        "![RNN scheme (Wiki)](https://upload.wikimedia.org/wikipedia/commons/thumb/b/b5/Recurrent_neural_network_unfold.svg/2560px-Recurrent_neural_network_unfold.svg.png)\n"
      ],
      "metadata": {
        "id": "eSqJHPB9fs7o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Exercise 3.1** Implement VanillaRNN\n",
        "\n"
      ],
      "metadata": {
        "id": "37BiKNEoKTzP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VanillaRNN(nn.Module):\n",
        "  def __init__(self, output_size, hidden_size, vocab_size, embed_size):\n",
        "    super(VanillaRNN, self).__init__()\n",
        "      ####################################################################\n",
        "    # Fill in missing code below (...),\n",
        "    # then remove or comment the line below to test your function\n",
        "    raise NotImplementedError(\"Vanilla RNN\")\n",
        "    ####################################################################\n",
        "    self.hidden_size = ...\n",
        "    self.neuron_embeddings = ...\n",
        "    self.rnn = ...\n",
        "    self.fc = ...\n",
        "\n",
        "    self.hidden_size = hidden_size\n",
        "    self.word_embeddings = nn.Embedding(vocab_size, embed_size)\n",
        "    self.rnn = nn.RNN(embed_size, hidden_size, num_layers=2)\n",
        "    self.fc = nn.Linear(2*hidden_size, output_size)\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    input = self.neuron_embeddings(inputs)\n",
        "    input = input.permute(1, 0, 2)\n",
        "    h_0 =  Variable(torch.zeros(2, input.size()[1], self.hidden_size)\n",
        "    output, h_n = self.rnn(input, h_0)\n",
        "    h_n = h_n.permute(1, 0, 2)\n",
        "    h_n = h_n.contiguous().view(h_n.size()[0], h_n.size()[1]*h_n.size()[2])\n",
        "    logits = self.fc(h_n)\n",
        "\n",
        "    return logits\n",
        "\n",
        "## Uncomment to test\n",
        "# sampleRNN = VanillaRNN(10, 50, 1000, 300)\n",
        "# print(sampleRNN)"
      ],
      "metadata": {
        "id": "7HQDROd1Ncr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VanillaRNN(nn.Module):\n",
        "  def __init__(self, output_size, hidden_size, vocab_size, embed_size):\n",
        "    super(VanillaRNN, self).__init__()\n",
        "      ####################################################################\n",
        "    # Fill in missing code below (...),\n",
        "    # then remove or comment the line below to test your function\n",
        "    raise NotImplementedError(\"Vanilla RNN\")\n",
        "    ####################################################################\n",
        "    self.hidden_size = ...\n",
        "    self.neuron_embeddings = ...\n",
        "    self.rnn = ...\n",
        "    self.fc = ...\n",
        "\n",
        "    self.hidden_size = hidden_size\n",
        "    self.word_embeddings = nn.Embedding(vocab_size, embed_size)\n",
        "    self.rnn = nn.RNN(embed_size, hidden_size, num_layers=2)\n",
        "    self.fc = nn.Linear(2*hidden_size, output_size)\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    input = self.neuron_embeddings(inputs)\n",
        "    input = input.permute(1, 0, 2)\n",
        "    h_0 =  Variable(torch.zeros(2, input.size()[1], self.hidden_size)\n",
        "    output, h_n = self.rnn(input, h_0)\n",
        "    h_n = h_n.permute(1, 0, 2)\n",
        "    h_n = h_n.contiguous().view(h_n.size()[0], h_n.size()[1]*h_n.size()[2])\n",
        "    logits = self.fc(h_n)\n",
        "\n",
        "    return logits\n",
        "\n",
        "## Uncomment to test\n",
        "# sampleRNN = VanillaRNN(10, 50, 1000, 300)\n",
        "# print(sampleRNN)"
      ],
      "metadata": {
        "id": "9AuUa09KKZ2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Exercise 3.2** Decoding behavior from sequential neural data using RNN\n",
        "\n",
        "Seq2seq"
      ],
      "metadata": {
        "id": "ZMC3LwFERfJs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 1:** Download the dataset. \n",
        "\n",
        "It is a classic monkey reach dataset with obstacles (MC_Maze) fro Churchland et al. ([more info](https://dandiarchive.org/dandiset/000140))"
      ],
      "metadata": {
        "id": "X_M8RYE-UVRG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ! pip install \"dandi>=0.13.0\"\n",
        "# ! dandi download DANDI:000140/0.220113.0408\n",
        "# ! pip install git+https://github.com/neurallatents/nlb_tools.git"
      ],
      "metadata": {
        "id": "FlTiaBXJUEhf"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nlb_tools.nwb_interface import NWBDataset\n",
        "\n",
        "# for simplicity, we are using NLB tools \n",
        "dataset = NWBDataset(\"/content/000140/sub-Jenkins\", \"*\", \n",
        "                     split_heldout=True)\n",
        "\n",
        "# to view the dataset, uncomment the next line\n",
        "# dataset.data"
      ],
      "metadata": {
        "id": "pRv1Bgq9UnC0"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 2**: Select the fields that we are going to use\n",
        "\n",
        "The continuous data provided with the MC_Maze datasets includes:\n",
        "\n",
        "* `cursor_pos` - x and y position of the cursor controlled by the monkey\n",
        "* `eye_pos` - x and y position of the monkey's point of gaze on the screen, in mm\n",
        "* `hand_pos` - x and y position of the monkey's hand, in mm\n",
        "* `hand_vel` - x and y velocities of the monkey's hand, in mm/s, computed offline using np.gradient\n",
        "* `spikes` - spike times binned at 1 ms\n",
        "\n",
        "Here we will try picking a single aspect of behavior (e.g. `hand_vel`) and decoding it from the spike data `spikes` using an RNN decoder.\n"
      ],
      "metadata": {
        "id": "Mol2pboBW5-w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 3:** Trialize and visualize the data\n",
        "Surprisingly autonomous -> movement onset"
      ],
      "metadata": {
        "id": "BhsJ5pjmY6r-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVNC6DrGW08_",
        "outputId": "4f7c2ace-6ac1-41a6-fef5-130c1a7ba801"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on class NWBDataset in module nlb_tools.nwb_interface:\n",
            "\n",
            "class NWBDataset(builtins.object)\n",
            " |  NWBDataset(fpath, prefix='', split_heldout=True, skip_fields=[])\n",
            " |  \n",
            " |  A class for loading/preprocessing data from NWB files for\n",
            " |  the NLB competition\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __init__(self, fpath, prefix='', split_heldout=True, skip_fields=[])\n",
            " |      Initializes an NWBDataset, loading data from \n",
            " |      the indicated file(s)\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      fpath : str\n",
            " |          Either the path to an NWB file or to a directory\n",
            " |          containing NWB files\n",
            " |      prefix : str, optional\n",
            " |          A pattern used to filter the NWB files in directory\n",
            " |          by name. By default, prefix='' loads all .nwb files in\n",
            " |          the directory. Please refer to documentation for\n",
            " |          the `glob` module for more details: \n",
            " |          https://docs.python.org/3/library/glob.html\n",
            " |      split_heldout : bool, optional\n",
            " |          Whether to load heldin units and heldout units\n",
            " |          to separate fields or not, by default True\n",
            " |      skip_fields : list, optional\n",
            " |          List of field names to skip during loading,\n",
            " |          which may be useful if memory is an issue.\n",
            " |          Field names must match the names automatically\n",
            " |          assigned in the loading process. Spiking data \n",
            " |          can not be skipped. Field names in the list\n",
            " |          that are not found in the dataset are\n",
            " |          ignored\n",
            " |  \n",
            " |  add_continuous_data(self, cts_data, signal_type, chan_names=None)\n",
            " |      Adds a continuous data field to the main DataFrame\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      cts_data : np.ndarray\n",
            " |          A numpy array whose first dimension matches the DataFrame \n",
            " |          at self.data\n",
            " |      signal_name : str\n",
            " |          The label for this group of signals\n",
            " |      chan_names : list of str, optional\n",
            " |          The channel names for this data\n",
            " |  \n",
            " |  add_trialized_data(self, trial_data, signal_type, chan_names=None)\n",
            " |      Adds a trialized data field to the main DataFrame\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      trial_data : pd.DataFrame\n",
            " |          A trial_data dataframe containing a data field\n",
            " |          that will be added to the continuous dataframe\n",
            " |      signal_type : str\n",
            " |          The label for the data to be added\n",
            " |      chan_names : list of str, optional\n",
            " |          The channel names for the data when added\n",
            " |  \n",
            " |  calculate_onset(self, field_name, onset_threshold, peak_prominence=0.1, peak_distance_s=0.1, multipeak_threshold=0.2)\n",
            " |      Calculates onset for a given field by finding \n",
            " |      peaks and threshold crossings. Developed for \n",
            " |      speed onset calculation\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      field_name : str\n",
            " |          The field to use for onset calculation, used\n",
            " |          with recursive getattr on self.data\n",
            " |      onset_threshold : float\n",
            " |          The threshold for onset as a percentage of the \n",
            " |          peak height\n",
            " |      peak_prominence : float, optional\n",
            " |          Minimum prominence of peaks. Passed to \n",
            " |          `scipy.signal.find_peaks`, by default 0.1\n",
            " |      peak_distance_s : float, optional\n",
            " |          Minimum distance between peaks. Passed to \n",
            " |          `scipy.signal.find_peaks`, by default 0.1\n",
            " |      multipeak_threshold : float, optional\n",
            " |          Subsequent peaks within a trial must be no \n",
            " |          larger than this percentage of the first peak, \n",
            " |          otherwise the onset calculation fails, by default 0.2\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      pd.Series\n",
            " |          The times of identified peaks\n",
            " |  \n",
            " |  load(self, fpath, split_heldout=True, skip_fields=[])\n",
            " |      Loads data from an NWB file into two dataframes,\n",
            " |      one for trial info and one for time-varying data\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      fpath : str\n",
            " |          Path to the NWB file\n",
            " |      split_heldout : bool, optional\n",
            " |          Whether to load heldin units and heldout units\n",
            " |          to separate fields or not, by default True\n",
            " |      skip_fields : list, optional\n",
            " |          List of field names to skip during loading,\n",
            " |          which may be useful if memory is an issue.\n",
            " |          Field names must match the names automatically\n",
            " |          assigned in the loading process. Spiking data \n",
            " |          can not be skipped. Field names in the list\n",
            " |          that are not found in the dataset are\n",
            " |          ignored\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      tuple\n",
            " |          Tuple containing a pd.DataFrame of continuous loaded\n",
            " |          data, a pd.DataFrame with trial metadata, a dict\n",
            " |          with descriptions of fields in the DataFrames, and\n",
            " |          the bin width of the loaded data in ms\n",
            " |  \n",
            " |  make_trial_data(self, start_field='start_time', end_field='end_time', align_field=None, align_range=(None, None), margin=0, ignored_trials=None, allow_overlap=False, allow_nans=False)\n",
            " |      Makes a DataFrame of trialized data based on \n",
            " |      an alignment field\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      start_field : str, optional\n",
            " |          The field in `trial_info` to use as the beginning of\n",
            " |          each trial, by default 'start_time'\n",
            " |      end_field : str, optional\n",
            " |          The field in `trial_info` to use as the end of each trial,\n",
            " |          by default 'end_time'\n",
            " |      align_field : str, optional\n",
            " |          The field in `trial_info` to use for alignment,\n",
            " |          by default None, which does not align trials and\n",
            " |          instead takes them in their entirety\n",
            " |      align_range : tuple of int, optional\n",
            " |          The offsets to add to the alignment field to \n",
            " |          calculate the alignment window, by default (None, None) \n",
            " |          uses `trial_start` and `trial_end`\n",
            " |      margin : int, optional\n",
            " |          The number of ms of extra data to include on either end of \n",
            " |          each trial, labeled with the `margin` column for easy \n",
            " |          removal. Margins are useful for decoding and smoothing\n",
            " |      ignored_trials : pd.Series or np.ndarray, optional\n",
            " |          A boolean pd.Series or np.ndarray of the same length \n",
            " |          as trial_info with True for the trials to ignore, by \n",
            " |          default None ignores no trials. This is useful for \n",
            " |          rejecting trials outside of the alignment process\n",
            " |      allow_overlap : bool, optional\n",
            " |          Whether to allow overlap between trials, by default False\n",
            " |          truncates each trial at the start of the subsequent trial\n",
            " |      allow_nans : bool, optional\n",
            " |          Whether to allow NaNs within trials, by default False\n",
            " |          drops all timestamps containing NaNs in any column\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      pd.DataFrame\n",
            " |          A DataFrame containing trialized data. It has the same\n",
            " |          fields as the continuous `self.data` DataFrame, but \n",
            " |          adds `trial_id`, `trial_time`, and `align_time`. It also\n",
            " |          resets the index so `clock_time` is a column rather than\n",
            " |          an index. This DataFrame can be pivoted to plot its \n",
            " |          various fields across trials, aligned relative to \n",
            " |          `align_time`, `trial_time`, or `clock_time`\n",
            " |  \n",
            " |  resample(self, target_bin)\n",
            " |      Rebins spikes and performs antialiasing + downsampling on \n",
            " |      continuous signals\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      target_bin : int\n",
            " |          The target bin size in milliseconds. Note that it must be an \n",
            " |          integer multiple of self.bin_width\n",
            " |  \n",
            " |  smooth_spk(self, gauss_width, signal_type=None, name=None, overwrite=False, ignore_nans=False, parallelized=True, dtype='float64')\n",
            " |      Applies Gaussian smoothing to the data. Most often\n",
            " |      applied to spikes\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      gauss_width : int\n",
            " |          The standard deviation of the Gaussian to use for\n",
            " |          smoothing, in ms\n",
            " |      signal_type : str or list of str, optional\n",
            " |          The group of signals to smooth, by default \n",
            " |          None, which smooths 'spikes' and 'heldout_spikes'\n",
            " |          if present in the DataFrame\n",
            " |      name : str, optional\n",
            " |          The name to use for the smoothed data when adding \n",
            " |          it back to the DataFrame, by default None. If\n",
            " |          provided, the new signal_type name will be \n",
            " |          the original name + '_' + `name`. Must be provided\n",
            " |          if overwrite is False\n",
            " |      overwrite : bool, optional\n",
            " |          Whether to overwrite the original data,\n",
            " |          by default False\n",
            " |      ignore_nans : bool, optional\n",
            " |          Whether to ignore NaNs when smoothing, by default\n",
            " |          False. When NaNs are not ignored, they propagate\n",
            " |          into valid data during convolution, but ignoring\n",
            " |          NaNs is much slower\n",
            " |      parallelized : bool, optional\n",
            " |          Whether to parallelize the smoothing operation \n",
            " |          with multiprocessing.Pool.map(). This may cause\n",
            " |          issues on certain systems, so it can be disabled\n",
            " |      dtype : str or dtype\n",
            " |          Data type for the smoothing output to be cast to,\n",
            " |          in case of memory issues or precision concerns.\n",
            " |          By default 'float64'. Only other float dtypes are\n",
            " |          recommended\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors defined here:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Exercise 3.3** Auto-encoding sequential neural data with RNN-based LFADS model "
      ],
      "metadata": {
        "id": "Z5wTVXsPR-kC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Exercise 3.4\\*:** Non-autonomous LFADS: identifiability of control inputs\n",
        "\n"
      ],
      "metadata": {
        "id": "OeQeYsaEHm7s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "\n",
        "1. Izhikevich, Eugene M. Dynamical systems in neuroscience. MIT press, 2007."
      ],
      "metadata": {
        "id": "kblnhvBQ9GvC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "pcrCyeCK9ICe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}